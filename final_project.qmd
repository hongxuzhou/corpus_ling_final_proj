---
title: "Stylistic Variations in Neil Gaiman's Work: A Corpus-Driven Analysis"
author: "Hongxu Zhou" 
---

## Introduction

As a prominent contemporary author, Neil Gaiman has produced a substantial body of work spanning diverse genres and targeting audiences ranging from children to adults. This project employs corpus-driven methods to analyse potential stylistic variations in Gaiman's writing that may distinguish works intended for different reader demographics. Through systematic linguistic analysis, the study aims to identify and characterize textual features that differentiate Gaiman's writing across audience-targeted works.

```{r}
#| label: packages_loading
#| message: false
#| warning: false
#| include: false
#| paged-print: false
# Load required packages
library(tidyverse)      
library(here)           
library(quanteda)       
library(quanteda.textstats)  
library(sentimentr)     
library(wordcloud)      
library(ggplot2)        
library(gridExtra)     
library(scales)        
library(ggridges)      
library(dplyr)
library(stopwords)
library(udpipe)
library(ggrepel) 
library(factoextra) 
```

```{r}
#| label: corpus_loading

# Read the corpus
gaiman_corpus <- readRDS(here("corpora", "gaiman_corpus_complete.rds"))

# Explore the structure
str(gaiman_corpus, max.level = 1)
```

The corpus contains three levels of the texts: document-level (tokenised by chapter); sentence-level (tokenised by sentence-end punctuation); and token-level (tokenised by word).

(I will put a table here showcasing the sizes of each book with basic statistics)

\
To accelerate the calculation and analysis, some basic statistical features are calculated beforehand and added to the corpus as the layers of Metadata and Vocabulary.

## Literature Review 

-   Previous corpus-based studies on stylistic variation in fiction

-   Research on linguistic markers of audience targeting (children's vs. adult literature)

-   Studies on Neil Gaiman's writing style (if available)

-   **Drafted Research Questions**:

    1.  What linguistic features differentiate Gaiman's writing across audience demographics?

## Methodology 

### Corpus 

-   **Data**: Description of the 10 books with their basic information (provide a table with titles, word counts, publication dates, and target audience)

-   **Structure**: Explanation of your three-tiered corpus design (document, sentence, token levels)

-   Description of metadata and vocabulary components

-   Preprocessing steps

### Analysis 

#### lexical level 

```{r}
#| label: exploratory_analysis
# Extract book-level metadata
book_metadata <- gaiman_corpus$metadata$book_level

# Display basic statistics about the corpus 
corpus_summary <- book_metadata |> 
  summarise(
    total_books = n(),
    total_words = sum(total_words),
    avg_words_per_book = mean(total_words),
    avg_chapters = mean(chapters),
    avg_sentences = mean(sentences),
    avg_sentence_length = mean(avg_sentence_length),
    avg_lexical_diversity = mean(lexical_diversity)
  )

print(corpus_summary)
```

It might be counter-intuitive to see that the shortest, very children-oriented painted book Fortunately, the Milk has the highest TTR, while the longer, adult-targeted books such as American Gods and Anasi Boys are the lowerest. However, it makes sense. Given that FTM is the shortest, the words are more likely to appear in fewer times, even only once, which increases the ratio of type. Being opposite to that, the long novels contain much more words of which many are common words. They water down TTR. This calls for more detailed and comprehensive stylistic analysis on multiple layers of the text.

##### Word Length Distribution

Moving on, we check the distribution of word length

```{r}
#| label: word_length_cal
#| message: false
#| warning: false
#| paged-print: false
# Calculate word length for all tokens
token_length <- gaiman_corpus$token_level |> 
  # Calculate character length of each token 
  mutate(word_length = nchar(token))

# With token length, we can calculate word lengh by book
word_lengh_stats <- token_length |> 
  group_by(book_id) |> 
  summarise(
    avg_length = mean(word_length),
    median_length = median(word_length),
    sd_length = sd(word_length),
    # We can further check the percentage of words by length 
    short_word_pct = mean(word_length <= 4) * 100, # 1-4 words = short
    medium_word_pct = mean(word_length > 4 & word_length <= 8)* 100, # 4 - 8 = medium
    long_word_pct = mean(word_length >8)*100, # 9+ words are long
    .groups = "drop"
  ) |> 
  arrange(desc(avg_length))

print(word_lengh_stats)
```

As the chart shows, Gaiman's writing shows a consistent pattern across his works. Most words are at around 4 characters, which reflect the common pattern of English.

We can visualise the results

```{r}
#| label: word_length_vis
word_lengh_dist <- token_length |> 
  # Count occurrences of each word length in each book 
  count(book_id, word_length) |> 
  # Group by book to calculate percentages 
  group_by(book_id) |> 
  mutate(percentage = n / sum(n) * 100) |> 
  ungroup() |> 
  # start vis
  ggplot(aes(x = word_length, 
             y = percentage,
             color = book_id,
             group = book_id)) +
  geom_line(linewidth = 1, alpha = 0.7) + # no more "size", update to linewidth
  labs(
    title = "Word Length Distribution by Book", 
    subtitle = "Percentage of words at each character length",
    x = "Word Length (by character)",
    Y = "Percentage of books",
    color = "Book"
    
  )+
  theme_classic() +
  theme(legend.position = "bottom")

print(word_lengh_dist)
```

##### Lexical Diversity

```{r}
#| label: ttr_visualisation
# Lexical diversity is already stored in the corpus, so we just need to visulaise it 
lex_div_plot <- ggplot(book_metadata, 
                      aes(x = reorder(book_id, lexical_diversity), 
                          y = lexical_diversity)) +
  geom_bar(stat = "identity", fill = "lightgreen", alpha = 0.8) +
  # Add reference line for average
  geom_hline(yintercept = mean(book_metadata$lexical_diversity), 
             linetype = "dashed", color = "red") +
  # Add text annotation for average
  annotate("text", 
           x = 1, 
           y = mean(book_metadata$lexical_diversity) + 0.01, 
           label = "Corpus Average", 
           hjust = 0, 
           color = "red") +
  labs(
    title = "Lexical Diversity by Book",
    subtitle = "Higher values indicate more varied vocabulary",
    x = "Book",
    y = "Type-Token Ratio"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(lex_div_plot)

```

```{r}
#| label: word_frequnecy
#| include: false
vocabulary <- gaiman_corpus$vocabulary
stopwords = stopwords(language = "en")

top_content_words <- vocabulary |> 
  filter(!(token_lower %in% stopwords)) |> 
  arrange(desc(frequency)) |> 
  head(30)

# Skip printing resutls, directly visualise the bar chart
top_words_plot <- ggplot(top_content_words,
                         aes(x = reorder(token_lower, frequency),
                             y = frequency)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  labs(
    title = "Top 30 Conent Words in Gaiman Corpus",
    subtitle = "Stopwords applied",
    x = "Word",
    y = "Frequency"
  ) + 
  theme_classic() +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))

print(top_words_plot)
```

```{r}
#| label: quanteda_loading
# Create a corpus from the document level data
quanteda_corpus <- corpus(
  gaiman_corpus$document_level$text,
  docnames = gaiman_corpus$document_level$doc_id
)

# Add document variables for grouping
docvars(quanteda_corpus, "book_id") <- gaiman_corpus$document_level$book_id
docvars(quanteda_corpus, "title") <- gaiman_corpus$document_level$title
docvars(quanteda_corpus, "chapter_num") <- gaiman_corpus$document_level$chapter_num

```

##### Keyword analysis

We use log-off to calculate keywords of the ten books

```{r}
#| label: log_off_cal
# Log odds function to compare one book against all others
# This identifies distinctive words in each book compared to other books
calculate_log_odds <- function(dfm_obj, target_book) {
  # Get index of target book
  target_idx <- which(docnames(dfm_obj) == target_book)
  
  # Skip if book not found
  if(length(target_idx) == 0) return(NULL)
  
  # Get counts for target book
  target_counts <- as.numeric(dfm_obj[target_idx,])
  target_total <- sum(target_counts)
  
  # Get counts for all other books
  others_counts <- colSums(dfm_obj[-target_idx,])
  others_total <- sum(others_counts)
  
  # Calculate log odds (adding small constant to prevent division by zero)
  epsilon <- 0.5  # Smoothing constant
  log_odds <- log((target_counts + epsilon) / (target_total - target_counts + epsilon)) - 
              log((others_counts + epsilon) / (others_total - others_counts + epsilon))
  
  # Create results data frame
  data.frame(
    term = colnames(dfm_obj),
    log_odds = log_odds,
    target_count = target_counts,
    others_count = others_counts,
    stringsAsFactors = FALSE
  ) %>%
    # Sort by log odds (descending)
    arrange(desc(log_odds))
}

# Create document-feature matrix
book_dfm <- tokens(quanteda_corpus) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_remove(pattern = "[\\d\\p{P}]", valuetype = "regex") %>% # Remove numbers and punctuation
  dfm() %>%
  dfm_trim(min_termfreq = 3) %>%  # Remove very rare terms
  dfm_group(groups = docvars(quanteda_corpus, "book_id"))

# Get log odds for each book
book_ids <- unique(docvars(quanteda_corpus, "book_id"))
all_keywords <- list()

for (book_id in book_ids) {
  # Calculate log odds
  book_keywords <- calculate_log_odds(book_dfm, book_id)
  
  # Skip if book not found in DFM
  if(is.null(book_keywords)) next
  
  # Keep top 20 distinctive words
  book_keywords <- book_keywords %>%
    head(20) %>%
    mutate(book_id = book_id)
  
  # Add to results list
  all_keywords[[book_id]] <- book_keywords
}

# Combine results
keywords_df <- bind_rows(all_keywords)
```

We can visualise the results:

```{r}
#| label: log_off_vis
# Visualize top 10 keywords for each book
top_keywords_plot <- keywords_df %>%
  group_by(book_id) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = tidytext::reorder_within(term, log_odds, book_id), 
             y = log_odds, 
             fill = book_id)) +
  geom_col() +
  facet_wrap(~ book_id, scales = "free_y") +
  upstartr::scale_x_reordered() +  # Required for reorder_within
  coord_flip() +
  labs(
    title = "Top 10 Distinctive Words by Book",
    subtitle = "Based on log odds ratio compared to other books",
    x = "Term",
    y = "Log Odds Ratio"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(top_keywords_plot)
```

#### Sentence Level 

##### Sentence length analysis

```{r}
#| label: sent_loading

# Prepare sentence data
sentence_data <- gaiman_corpus$sentence_level %>%
  select(book_id, title, chapter_num, sentence_id, text)
```

```{r}
#| label: sentent_length_dist_vis
# Calculate sentence length statistics
sentence_length_stats <- sentence_data %>%
  # Add sentence length in words
  mutate(sentence_length = sapply(strsplit(text, "\\s+"), length)) %>%
  # Group by book
  group_by(book_id) %>%
  # Calculate statistics
  summarise(
    avg_length = mean(sentence_length),
    median_length = median(sentence_length),
    sd_length = sd(sentence_length),
    min_length = min(sentence_length),
    max_length = max(sentence_length),
    short_sent_pct = mean(sentence_length <= 5) * 100,  # Percentage of short sentences
    long_sent_pct = mean(sentence_length > 20) * 100    # Percentage of long sentences
  ) %>%
  arrange(desc(avg_length))

# Visualize sentence length distribution
sentence_length_vis <- sentence_data %>%
  # Add sentence length in words
  mutate(sentence_length = sapply(strsplit(text, "\\s+"), length)) %>%
  # Remove extreme outliers for better visualization
  filter(sentence_length <= 50) %>%
  ggplot(aes(x = sentence_length, y = book_id, fill = book_id)) +
  geom_density_ridges(alpha = 0.7, scale = 3) +
  labs(
    title = "Sentence Length Distribution by Book",
    subtitle = "Ridgeline plot of word counts per sentence",
    x = "Words per Sentence",
    y = "Book"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(sentence_length_vis)
```

##### Syntactic Complexity

Mean Dependency Length Analysis: the average distance between words and their syntactic heads

```{r}
#| label: ud_cal


# 1. Function to calculate mean dependency length for parsed text
calculate_mean_dep_length <- function(parsed_data) {
  # For each sentence, calculate dependency lengths
  parsed_data %>%
    # Filter out root nodes and punctuation
    filter(head_token_id != 0, upos != "PUNCT") %>%
    # Calculate absolute distance between each word and its head
    mutate(
      # Convert IDs to numeric and calculate distance
      token_id_num = as.numeric(token_id),
      head_id_num = as.numeric(head_token_id),
      # Absolute difference is the dependency length
      dep_length = abs(token_id_num - head_id_num)
    ) %>%
    # Calculate mean dependency length
    summarize(
      mean_dep_length = mean(dep_length, na.rm = TRUE),
      median_dep_length = median(dep_length, na.rm = TRUE),
      max_dep_length = max(dep_length, na.rm = TRUE),
      deps_analyzed = n(),
      .groups = "drop"
    )
}

# 2. Process a sample of text from each book to calculate dependency lengths
# This avoids processing the entire corpus which could be time-consuming
set.seed(42) # For reproducibility in sampling
sample_size <- 100 # Sample 100 sentences per book for analysis

# Get sample sentences from each book - FIXED version
sampled_sentences <- sentence_data %>%
  # Split by book_id
  group_by(book_id) %>%
  # Use group_modify to properly access group size
  group_modify(~{
    # Get the minimum of sample_size or actual group size
    sample_size_to_use <- min(sample_size, nrow(.x))
    # Sample that many rows
    slice_sample(.x, n = sample_size_to_use)
  }) %>%
  ungroup()

# Create a unique identifier for each sampled sentence
sampled_sentences <- sampled_sentences %>%
  mutate(sample_id = paste0(book_id, "_", row_number()))

# Initialize dependency parsing model
# Note: This requires downloading a model - this block shows expected code flow
dep_model <- udpipe_download_model(language = "english-ewt", model_dir = getwd())
ud_model <- udpipe_load_model(dep_model$file_model)

# Process the sample sentences with udpipe for dependency parsing
# Using the sample_id as doc_id to maintain uniqueness
parsed_samples <- udpipe(
  x = sampled_sentences$text, 
  object = ud_model,
  doc_id = sampled_sentences$sample_id,  # Use our unique sample_id
  parallel.cores = 1
)

# Create a mapping between sample_id and book_id
sample_book_mapping <- sampled_sentences %>%
  select(sample_id, book_id, title) %>%
  distinct()

# 3. Calculate mean dependency length for each sentence, then aggregate by book
sentence_dependency_stats <- parsed_samples %>%
  # Group by doc_id (which is our sample_id)
  group_by(doc_id) %>%
  # Calculate dependency stats for each sentence
  group_modify(~calculate_mean_dep_length(.x))

# Join with our mapping to get book information
book_dependency_stats <- sentence_dependency_stats %>%
  # Join with mapping to get book_id
  left_join(sample_book_mapping, by = c("doc_id" = "sample_id")) %>%
  # Group by book to aggregate sentence-level stats
  group_by(book_id, title) %>%
  # Calculate book-level averages
  summarize(
    mean_dep_length = mean(mean_dep_length, na.rm = TRUE),
    median_dep_length = mean(median_dep_length, na.rm = TRUE),
    max_dep_length = max(max_dep_length, na.rm = TRUE),
    sentences_analyzed = n(),
    .groups = "drop"
  )

```

```{r}
#| label: ud_vis

# 4. Create visualization of mean dependency length
dep_length_plot <- ggplot(book_dependency_stats,
  # Sort by mean dependency length
  aes(x = reorder(book_id, mean_dep_length),
      y = mean_dep_length)) +
  geom_col(fill = "#5D93E1") +
  # Add error bars showing variability
  geom_errorbar(
    aes(ymin = mean_dep_length - median_dep_length/4, 
        ymax = mean_dep_length + median_dep_length/4),
    width = 0.2, color = "#2C528C"
  ) +
  labs(
    title = "Syntactic Complexity in Gaiman's Works",
    subtitle = "Mean dependency length (average distance between related words)",
    x = "Book",
    y = "Mean Dependency Length"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )

# 5. Find illustrative examples of short and long dependencies
# We'll find examples from the original parsed sentences
parsed_with_dep_length <- parsed_samples %>%
  # Calculate dependency length for each token
  mutate(
    token_id_num = as.numeric(token_id),
    head_id_num = as.numeric(head_token_id),
    dep_length = abs(token_id_num - head_id_num)
  )

# Aggregate to get max dependency length in each sentence
sentence_max_deps <- parsed_with_dep_length %>%
  group_by(doc_id, sentence_id) %>%
  summarize(
    max_dep = max(dep_length[upos != "PUNCT" & head_token_id != "0"], na.rm = TRUE),
    sentence_text = first(sentence),
    .groups = "drop"
  ) %>%
  # Remove potential NA/Inf values from empty sentences
  filter(is.finite(max_dep))

# Find better examples
short_example <- sentence_max_deps %>%
  filter(max_dep > 1 & max_dep <= 3) %>%  # Not too trivial, but still simple
  filter(nchar(sentence_text) > 10) %>%    # Ensure it's a real sentence
  arrange(max_dep) %>%
  slice(1)

long_example <- sentence_max_deps %>%
  filter(max_dep >= 7) %>%                # Clearly complex
  arrange(desc(max_dep)) %>%
  slice(1)

# Join with book information for examples
example_sentences <- bind_rows(
  short_example %>% mutate(type = "Short dependency"),
  long_example %>% mutate(type = "Long dependency")
) %>%
  left_join(sample_book_mapping, by = c("doc_id" = "sample_id"))

# Print results
print(dep_length_plot)

# Print example sentences with book sources
cat("\nExample sentences illustrating dependency length:\n\n")

for(i in 1:nrow(example_sentences)) {
  cat(paste0(example_sentences$type[i], " (max = ", 
             round(example_sentences$max_dep[i], 1), 
             ") from ", example_sentences$title[i], ":\n"))
  cat(example_sentences$sentence_text[i], "\n\n")
}
```

##### Readability Analysis

```{r}
#| label: readability_score_vis
# Data-driven Readability Analysis
# ---------------------------------------
# Focus solely on Flesch-Kincaid Grade Level without imposing age categories

# 1. Calculate readability scores for each book
book_texts <- gaiman_corpus$document_level %>%
  group_by(book_id) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup()

# 2. Use quanteda for readability calculations
readability_scores <- textstat_readability(
  book_texts$text,
  measure = c("Flesch.Kincaid")  # Focus on just this measure
)
readability_scores$book_id <- book_texts$book_id

# 3. Join with book information
readability_data <- readability_scores %>%
  # Get book information (title only)
  left_join(
    gaiman_corpus$metadata$book_level %>% 
      select(book_id, title), 
    by = "book_id"
  )

# 4. Create a cleaner, data-driven visualization
readability_plot <- ggplot(readability_data,
  # Sort by Flesch-Kincaid score (reading difficulty)
  aes(x = reorder(book_id, Flesch.Kincaid),
      y = Flesch.Kincaid)) +
  # Use a gradient color reflecting complexity, but not imposing categories
  geom_col(aes(fill = Flesch.Kincaid)) +
  scale_fill_gradient(low = "#E3F2FD", high = "#1565C0") +
  # Add labeled reference lines for grade level interpretation
  geom_hline(yintercept = c(5, 8, 12), linetype = "dashed", color = "gray60") +
  annotate("text", x = 1, y = 5.2, label = "5th Grade", hjust = 0, size = 3, color = "gray40") +
  annotate("text", x = 1, y = 8.2, label = "8th Grade", hjust = 0, size = 3, color = "gray40") +
  annotate("text", x = 1, y = 12.2, label = "12th Grade", hjust = 0, size = 3, color = "gray40") +
  labs(
    title = "Reading Difficulty of Gaiman's Works",
    subtitle = "Flesch-Kincaid Grade Level (higher = more difficult reading)",
    x = "Book",
    y = "Reading Grade Level",
    fill = "Grade Level"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank(),
    legend.position = "right"
  )

# 5. Add book titles to the plot for clearer identification
readability_plot <- readability_plot +
  # Add book titles as text labels
  geom_text(
    aes(label = title, y = 0.5),
    angle = 90, hjust = 0, size = 2.5, color = "grey45"
  )

# Display the visualization
print(readability_plot)
```

##### Sentiment & Profanity Analysis

I use sentenceR package to conduct sentence-level sentiment analysis

1.  sentiment analysis

```{r}
#| label: senti_ana_vis

# First, let's set up a simple analysis pipeline

# 1. Calculate sentence-level sentiment
sentence_sentiment <- sentiment_by(
  text.var = sentence_data$text,
  by = list(
    book_id = sentence_data$book_id,
    sentence_id = sentence_data$sentence_id
  )
)

# 2. Classify each sentence as positive, negative, or neutral
sentence_sentiment <- sentence_sentiment %>%
  mutate(
    sentiment_class = case_when(
      ave_sentiment > 0.05 ~ "Positive",
      ave_sentiment < -0.05 ~ "Negative",
      TRUE ~ "Neutral"  # Use a small buffer around zero for neutral
    )
  )

# 3. Calculate the distribution for each book
book_sentiment_distribution <- sentence_sentiment %>%
  group_by(book_id) %>%
  summarize(
    total_sentences = n(),
    positive_count = sum(sentiment_class == "Positive"),
    negative_count = sum(sentiment_class == "Negative"),
    neutral_count = sum(sentiment_class == "Neutral"),
    positive_pct = (positive_count / total_sentences) * 100,
    negative_pct = (negative_count / total_sentences) * 100,
    neutral_pct = (neutral_count / total_sentences) * 100
  )

# 4. Verify our percentages add up to 100%
book_sentiment_distribution <- book_sentiment_distribution %>%
  mutate(total_pct = positive_pct + negative_pct + neutral_pct)

# Check results (should be close to 100% for each book)
print(book_sentiment_distribution[, c("book_id", "total_pct")])

# 5. Create data for visualization
sentiment_long <- book_sentiment_distribution %>%
  select(book_id, positive_pct, negative_pct, neutral_pct) %>%
  pivot_longer(
    cols = c("positive_pct", "negative_pct", "neutral_pct"),
    names_to = "sentiment",
    values_to = "percentage"
  ) %>%
  mutate(
    sentiment = case_when(
      sentiment == "positive_pct" ~ "Positive",
      sentiment == "negative_pct" ~ "Negative",
      sentiment == "neutral_pct" ~ "Neutral"
    )
  )

# 6. Create a simple stacked bar chart
sentiment_distribution_plot <- ggplot(sentiment_long, 
                                     aes(x = book_id, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(values = c(
    "Positive" = "#4CAF50",
    "Negative" = "#F44336",
    "Neutral" = "#9E9E9E"
  )) +
  labs(
    title = "Sentiment Distribution in Gaiman's Works",
    subtitle = "Percentage of positive, negative, and neutral sentences in each book",
    x = "Book",
    y = "Percentage of Sentences",
    fill = "Sentiment"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  # Ensure y-axis goes from 0-100%
  scale_y_continuous(limits = c(0, 100))

# Display the plot
print(sentiment_distribution_plot)

# Optional: Generate a summary table for reporting
sentiment_summary <- book_sentiment_distribution %>%
  select(book_id, positive_pct, negative_pct, neutral_pct) %>%
  arrange(desc(positive_pct))

print(sentiment_summary)
```

```{r}
#| label: senti_arc_vis
# Sentiment Flow Analysis
# This function visualizes how sentiment changes throughout each book's narrative progression

# 1. Calculate sentiment by chapter for each book
chapter_sentiment <- sentence_sentiment %>%
  # Join with document information to get chapter numbers
  left_join(
    gaiman_corpus$sentence_level %>% 
      select(book_id, sentence_id, chapter_num) %>% 
      distinct(),
    by = c("book_id", "sentence_id")
  ) %>%
  # Group by book and chapter to calculate average sentiment
  group_by(book_id, chapter_num) %>%
  summarize(
    chapter_sentiment = mean(ave_sentiment, na.rm = TRUE),
    sentences_count = n(),
    .groups = "drop"
  )

# 2. Calculate normalized position for each chapter within its book
normalized_sentiment_flow <- chapter_sentiment %>%
  # Group by book to calculate relative positions
  group_by(book_id) %>%
  mutate(
    # Get the max chapter number for each book
    max_chapter = max(chapter_num, na.rm = TRUE),
    # Calculate normalized position (0-100% of narrative)
    narrative_position = (chapter_num / max_chapter) * 100
  ) %>%
  # Get book titles for better labeling
  left_join(
    gaiman_corpus$document_level %>% 
      select(book_id, title) %>% 
      distinct(),
    by = "book_id"
  ) %>%
  # Make sure we don't have NA values
  filter(!is.na(chapter_sentiment))

# 3. Create a combined visualization showing all books' arcs
emotional_arcs_plot <- ggplot(normalized_sentiment_flow,
                             aes(x = narrative_position,
                                 y = chapter_sentiment,
                                 color = book_id)) +
  # Add smoothed trend lines to see the emotional arc
  geom_smooth(method = "loess", se = FALSE, span = 0.6, linewidth = 1) +
  # Add small points for actual chapter sentiments
  geom_point(alpha = 0.4, size = 1.5) +
  # Add horizontal reference line at neutral sentiment (0)
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", linewidth = 0.5) +
  # Add labels
  labs(
    title = "Emotional Arcs in Neil Gaiman's Works",
    subtitle = "Sentiment progression throughout narrative (0% = beginning, 100% = end)",
    x = "Narrative Progression (%)",
    y = "Average Sentiment Score",
    color = "Book"
  ) +
  # Use a color palette that distinguishes between books
  scale_color_brewer(palette = "Paired") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank()
  )

# 4. Create a facetted version showing individual book arcs
individual_arcs_plot <- ggplot(normalized_sentiment_flow,
                              aes(x = narrative_position,
                                  y = chapter_sentiment)) +
  # Add smoothed trend lines to see the emotional arc
  geom_smooth(method = "loess", se = FALSE, span = 0.6, aes(color = book_id), linewidth = 1) +
  # Add small points for actual chapter sentiments
  geom_point(alpha = 0.4, size = 1.5, color = "darkgray") +
  # Add horizontal reference line at neutral sentiment (0)
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", linewidth = 0.5) +
  # Facet by book title for individual arcs
  facet_wrap(~ title, scales = "free_y") +
  # Add labels
  labs(
    title = "Individual Emotional Arcs in Neil Gaiman's Works",
    subtitle = "Sentiment progression by narrative position for each book",
    x = "Narrative Progression (%)",
    y = "Average Sentiment Score"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    panel.spacing = unit(1, "lines"),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# 5. Print both visualizations
print(emotional_arcs_plot)
print(individual_arcs_plot)

# 6. Calculate key statistics about the arcs
arc_statistics <- normalized_sentiment_flow %>%
  group_by(book_id, title) %>%
  summarize(
    start_sentiment = first(chapter_sentiment),
    end_sentiment = last(chapter_sentiment),
    min_sentiment = min(chapter_sentiment),
    max_sentiment = max(chapter_sentiment),
    sentiment_range = max_sentiment - min_sentiment,
    overall_trend = end_sentiment - start_sentiment,
    .groups = "drop"
  ) %>%
  mutate(
    arc_type = case_when(
      overall_trend > 0.05 ~ "Rising (positive resolution)",
      overall_trend < -0.05 ~ "Falling (negative resolution)",
      TRUE ~ "Neutral/Circular"
    )
  ) %>%
  arrange(desc(overall_trend))

# 7. Print summary statistics
print(arc_statistics)
```

2.  Profanity analysis \
    I use dictionary based approach to detect the occurrence of different swear words in Gaiman's books.

```{r}
#| label: swear_analysis_calculation

# 2. Define categorized profanity dictionaries
# Expanded lists with more comprehensive coverage
mild_profanity <- c(
  "damn", "darn", "hell", "crap", "suck", "stupid", "idiot", "dumb", 
  "moron", "fool", "jerk", "dork", "heck", "gosh", "jeez", "Christ", 
  "God", "Jesus", "bloody", "blast", "drat", "freaking", "frigging",
  "poop", "butt", "arse", "bollocks", "turd", "bum"
)

moderate_profanity <- c(
  "ass", "asshole", "bastard", "shit", "bullshit", "piss", 
  "screw", "screwed", "dick", "cock", "prick", "slut", "whore", 
  "wanker", "tosser", "twat", "crap", "balls", "nuts", "pissed", 
  "fart", "jackass", "douche", "douchebag", "tits", "boobs"
)

strong_profanity <- c(
  "fuck", "fucked", "fucker", "fucking", "motherfucker", "motherfucking",
  "cunt", "pussy", "cock", "bitch", "cum", "jizz", "nigger", "faggot", 
  "fag", "spic", "chink", "kike", "retard", "whore", "slut"
)

# Combine all for overall profanity detection
all_profanity <- unique(c(mild_profanity, moderate_profanity, strong_profanity))

# 3. Prepare sentence-level data
sentence_data <- gaiman_corpus$sentence_level %>%
  select(book_id, title, sentence_id, text)

# 4. Apply overall profanity detection 
profanity_results <- profanity_by(
  text.var = sentence_data$text,
  by = sentence_data$book_id,
  profanity_list = all_profanity
)

# 5. Create function to count profanity by category
count_profanity_by_category <- function(book_id, sentence_data) {
  # Extract text for this book
  book_text <- sentence_data %>%
    filter(book_id == !!book_id) %>%
    pull(text) %>%
    paste(collapse = " ") %>%
    tolower()
  
  # Count instances of each category
  mild_count <- sum(sapply(mild_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  moderate_count <- sum(sapply(moderate_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  strong_count <- sum(sapply(strong_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  # Return counts by category
  return(c(
    mild = mild_count,
    moderate = moderate_count,
    strong = strong_count,
    total = mild_count + moderate_count + strong_count
  ))
}

# 6. Apply category counting to each book
book_ids <- unique(sentence_data$book_id)
category_counts <- lapply(book_ids, function(id) {
  counts <- count_profanity_by_category(id, sentence_data)
  data.frame(
    book_id = id,
    mild_count = counts["mild"],
    moderate_count = counts["moderate"],
    strong_count = counts["strong"],
    total_count = counts["total"]
  )
})

# Combine results
profanity_categories <- bind_rows(category_counts) %>%
  # Join with word counts from profanity_results
  left_join(
    profanity_results %>% select(book_id, word_count),
    by = "book_id"
  ) %>%
  # Calculate rates per 1000 words
  mutate(
    mild_per_1000 = (mild_count / word_count) * 1000,
    moderate_per_1000 = (moderate_count / word_count) * 1000,
    strong_per_1000 = (strong_count / word_count) * 1000,
    total_per_1000 = (total_count / word_count) * 1000
  ) %>%
  # Sort by total profanity rate
  arrange(desc(total_per_1000))

# 7. Create visualization of profanity categories
# Prepare data for stacked bar chart
profanity_long <- profanity_categories %>%
  select(book_id, ends_with("per_1000")) %>%
  # Keep only category columns
  select(-total_per_1000) %>%
  # Convert to long format
  pivot_longer(
    cols = ends_with("per_1000"),
    names_to = "category",
    values_to = "rate_per_1000"
  ) %>%
  # Clean up category names
  mutate(
    category = case_when(
      category == "mild_per_1000" ~ "Mild",
      category == "moderate_per_1000" ~ "Moderate", 
      category == "strong_per_1000" ~ "Strong"
    ),
    # Convert to factor with desired order
    category = factor(category, levels = c("Mild", "Moderate", "Strong"))
  )

# Create stacked bar chart
category_plot <- ggplot(profanity_long, 
                       aes(x = reorder(book_id, rate_per_1000, sum), 
                           y = rate_per_1000,
                           fill = category)) +
  geom_col() +
  scale_fill_manual(values = c(
    "Mild" = "#FFC107",
    "Moderate" = "#FF9800", 
    "Strong" = "#F44336"
  )) +
  labs(
    title = "Profanity in Gaiman's Works by Intensity Level",
    subtitle = "Words per 1,000 categorized by severity",
    x = "Book",
    y = "Frequency (per 1,000 words)",
    fill = "Profanity Level"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )

# 8. Function to extract examples for each category
extract_category_examples <- function(book_id, category, word_list, n_examples = 2) {
  # Get sentences for this book
  book_sentences <- sentence_data %>%
    filter(book_id == !!book_id)
  
  # Find sentences containing words from the specified category
  matches <- list()
  for (term in word_list) {
    pattern <- paste0("\\b", term, "\\b")
    for (i in 1:nrow(book_sentences)) {
      text <- book_sentences$text[i]
      if (str_detect(tolower(text), pattern)) {
        # Create highlighted text with term in asterisks
        highlighted <- str_replace_all(
          text, 
          regex(pattern, ignore_case = TRUE),
          paste0("**", term, "**")
        )
        matches <- c(matches, list(list(
          sentence_id = book_sentences$sentence_id[i],
          text = highlighted,
          term = term
        )))
        # Stop if we have enough examples
        if (length(matches) >= n_examples) break
      }
    }
    # Stop if we have enough examples
    if (length(matches) >= n_examples) break
  }
  
  # Convert list to data frame
  if (length(matches) > 0) {
    result <- do.call(rbind, lapply(matches, function(x) {
      data.frame(
        sentence_id = x$sentence_id,
        text = x$text,
        term = x$term,
        stringsAsFactors = FALSE
      )
    }))
    return(head(result, n_examples))
  } else {
    return(data.frame(
      sentence_id = integer(0),
      text = character(0),
      term = character(0)
    ))
  }
}

# 9. Extract examples for top books
# Get top 3 books by total profanity
top_books <- profanity_categories %>%
  top_n(3, total_per_1000) %>%
  pull(book_id)

# Extract examples for each category from each top book
profanity_examples <- list()
for (book in top_books) {
  profanity_examples[[book]] <- list(
    mild = extract_category_examples(book, "Mild", mild_profanity),
    moderate = extract_category_examples(book, "Moderate", moderate_profanity),
    strong = extract_category_examples(book, "Strong", strong_profanity)
  )
}


```

```{r}
#| label: swear_analysis_vis


# 10. Display results
print(category_plot)


```

```{r}
#| label: swear_analysis_stats
#| 
# Print category statistics
cat("\nProfanity statistics by category and book:\n")
print(profanity_categories %>% 
      select(book_id, mild_per_1000, moderate_per_1000, strong_per_1000, total_per_1000))

```

```{r}
#| label: swear_analysis_examples
# 11. Print examples with category context
cat("\nExamples of profanity by category from top books:\n\n")
for (book in top_books) {
  cat("===== Examples from", book, "=====\n")
  
  # Print mild examples
  cat("\nMILD PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$mild
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No mild profanity examples found.\n")
  }
  
  # Print moderate examples
  cat("\nMODERATE PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$moderate
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No moderate profanity examples found.\n")
  }
  
  # Print strong examples
  cat("\nSTRONG PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$strong
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No strong profanity examples found.\n")
  }
  
  cat("\n")
}
```

```{}
```

#### Discourse level

##### Point of View analysis

```{r}
#| label: pov_dist
# Function to analyze POV markers in text
analyze_pov <- function(text) {
  # Define POV marker patterns
  first_person <- "\\b(I|me|my|mine|myself|we|us|our|ours|ourselves)\\b"
  second_person <- "\\b(you|your|yours|yourself|yourselves)\\b"
  third_person_m <- "\\b(he|him|his|himself)\\b"
  third_person_f <- "\\b(she|her|hers|herself)\\b"
  third_person_n <- "\\b(it|its|itself)\\b"
  third_person_p <- "\\b(they|them|their|theirs|themselves)\\b"
  
  # Count matches (case insensitive)
  first <- str_count(tolower(text), first_person)
  second <- str_count(tolower(text), second_person)
  third_m <- str_count(tolower(text), third_person_m)
  third_f <- str_count(tolower(text), third_person_f)
  third_n <- str_count(tolower(text), third_person_n)
  third_p <- str_count(tolower(text), third_person_p)
  
  # Calculate totals
  first_total <- first
  second_total <- second
  third_total <- third_m + third_f + third_n + third_p
  
  # Return counts
  return(c(
    first_person = first_total,
    second_person = second_total,
    third_person = third_total,
    third_male = third_m,
    third_female = third_f,
    third_neutral = third_n,
    third_plural = third_p
  ))
}

# Apply to book texts
pov_results <- book_texts %>%
  rowwise() %>%
  mutate(pov_data = list(analyze_pov(text))) %>%
  ungroup() %>%
  mutate(
    first_person = map_dbl(pov_data, ~ .x["first_person"]),
    second_person = map_dbl(pov_data, ~ .x["second_person"]),
    third_person = map_dbl(pov_data, ~ .x["third_person"]),
    third_male = map_dbl(pov_data, ~ .x["third_male"]),
    third_female = map_dbl(pov_data, ~ .x["third_female"]),
    third_neutral = map_dbl(pov_data, ~ .x["third_neutral"]),
    third_plural = map_dbl(pov_data, ~ .x["third_plural"]),
    total_words = str_count(text, "\\S+"),
    # Calculate percentages
    first_pct = first_person / total_words * 100,
    second_pct = second_person / total_words * 100,
    third_pct = third_person / total_words * 100
  ) %>%
  select(-pov_data, -text)

# Visualize POV distribution
pov_long <- pov_results %>%
  select(book_id, first_pct, second_pct, third_pct) %>%
  pivot_longer(
    cols = c(first_pct, second_pct, third_pct),
    names_to = "pov",
    values_to = "percentage"
  ) %>%
  mutate(
    pov = case_when(
      pov == "first_pct" ~ "First Person",
      pov == "second_pct" ~ "Second Person",
      pov == "third_pct" ~ "Third Person"
    )
  )

pov_plot <- ggplot(pov_long, 
                  aes(x = book_id, y = percentage, fill = pov)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Point of View Distribution by Book",
    subtitle = "Percentage of personal pronouns by category",
    x = "Book",
    y = "Percentage of Total Words",
    fill = "Point of View"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(pov_plot)
```

```{r}
#| label: pov_shift_analysis
 # POV Stability Analysis (Window-based approach)
# ---------------------------------------

# First, check what columns are actually available in your token data
# str(gaiman_corpus$token_level)

# Adjust the analysis to work with your corpus structure
pov_stability <- gaiman_corpus$token_level %>%
  # Filter for pronoun tokens using token_lower instead of upos
  filter(token_lower %in% c(
    # First person
    "i", "me", "my", "mine", "myself", "we", "us", "our", "ours", "ourselves",
    # Second person
    "you", "your", "yours", "yourself", "yourselves",
    # Third person
    "he", "him", "his", "himself", "she", "her", "hers", "herself",
    "it", "its", "itself", "they", "them", "their", "theirs", "themselves"
  )) %>%
  # Add perspective classification
  mutate(
    perspective = case_when(
      token_lower %in% c("i", "me", "my", "mine", "myself", "we", "us", "our", "ours", "ourselves") ~ "first",
      token_lower %in% c("you", "your", "yours", "yourself", "yourselves") ~ "second",
      TRUE ~ "third"
    )
  ) %>%
  # Group by book and sentence
  group_by(book_id, sentence_id) %>%
  # Determine predominant perspective for each sentence
  summarize(
    perspective = case_when(
      sum(perspective == "first") > 0 ~ "first",
      sum(perspective == "second") > 0 ~ "second",
      sum(perspective == "third") > 0 ~ "third",
      TRUE ~ "none"  # Fallback (shouldn't happen given our filter)
    ),
    .groups = "drop"
  ) %>%
  # Group sentences into windows to avoid name-pronoun alternation issues
  group_by(book_id) %>%
  mutate(
    # Create perspective windows (groups of consecutive sentences)
    window_id = ceiling(row_number() / 5)
  ) %>%
  # Get dominant perspective for each window
  group_by(book_id, window_id) %>%
  summarize(
    dominant_perspective = names(which.max(table(perspective))),
    .groups = "drop"
  ) %>%
  # Track shifts in dominant perspective
  group_by(book_id) %>%
  mutate(
    prev_perspective = lag(dominant_perspective),
    is_shift = dominant_perspective != prev_perspective & !is.na(prev_perspective)
  ) %>%
  # Calculate stability index and perspective composition
  summarize(
    windows = n(),
    shifts = sum(is_shift, na.rm = TRUE),
    stability_index = 1 - (shifts / (windows - 1)),  # Higher = more stable
    pct_first = mean(dominant_perspective == "first", na.rm = TRUE) * 100,
    pct_second = mean(dominant_perspective == "second", na.rm = TRUE) * 100,
    pct_third = mean(dominant_perspective == "third", na.rm = TRUE) * 100,
    .groups = "drop"
  )

# Get book titles for better visualization
pov_stability <- pov_stability %>%
  left_join(
    gaiman_corpus$document_level %>% 
      select(book_id, title) %>% 
      distinct(),
    by = "book_id"
  )


```

```{r}
#| label: pov_stable_vis
# Create perspective stability visualization
stability_plot <- ggplot(pov_stability, 
                        aes(x = reorder(book_id, stability_index),
                            y = stability_index)) +
  geom_col(fill = "#6D9EC1") +
  geom_text(aes(label = sprintf("%.2f", stability_index)), 
            vjust = -0.5, size = 3) +
  labs(
    title = "Narrative Perspective Stability in Gaiman's Works",
    subtitle = "Higher values indicate more consistent perspective throughout the work",
    x = "Book",
    y = "Perspective Stability Index"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Display both visualizations
print(stability_plot)

```

```{r}
#| label: pov_composition_vis
# Create perspective composition visualization
composition_data <- pov_stability %>%
  select(book_id, pct_first, pct_second, pct_third) %>%
  pivot_longer(
    cols = c(pct_first, pct_second, pct_third),
    names_to = "perspective",
    values_to = "percentage"
  ) %>%
  mutate(
    perspective = case_when(
      perspective == "pct_first" ~ "First Person",
      perspective == "pct_second" ~ "Second Person",
      perspective == "pct_third" ~ "Third Person"
    )
  )

composition_plot <- ggplot(composition_data,
                          aes(x = reorder(book_id, percentage, sum),
                              y = percentage,
                              fill = perspective)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("First Person" = "#E57373",
                               "Second Person" = "#FFB74D",
                               "Third Person" = "#81C784")) +
  labs(
    title = "Narrative Perspective Composition in Gaiman's Works",
    subtitle = "Percentage of narrative told in each perspective",
    x = "Book",
    y = "Percentage",
    fill = "Perspective"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(composition_plot)
```

##### Modal verbs analysis

```{r}
#| label: mocal_verb_cal
# Modal Verb Analysis by Narrative Function
# ---------------------------------------

# Prepare book texts - create if not already available
book_texts <- gaiman_corpus$document_level %>%
  group_by(book_id) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup()

# Corrected function to categorize modal verbs by narrative function
analyze_modal_functions <- function(text) {
  # Define patterns for modal categories
  world_building_pattern <- "\\b(must|should|have to|has to|had to|always|never)\\b"
  possibility_pattern <- "\\b(may|might|could|can|maybe|perhaps)\\b"
  hypothetical_pattern <- "\\b(would|if|whether)\\b"
  intent_pattern <- "\\b(will|shall|going to|want to)\\b"
  
  # Count matches for each category
  world_count <- str_count(tolower(text), world_building_pattern)
  possibility_count <- str_count(tolower(text), possibility_pattern)
  hypothetical_count <- str_count(tolower(text), hypothetical_pattern)
  intent_count <- str_count(tolower(text), intent_pattern)
  
  # Return named vector of counts
  return(c(
    world_building = world_count,
    possibility = possibility_count,
    hypothetical = hypothetical_count,
    intent = intent_count
  ))
}

# Apply function to each book
modal_results <- book_texts %>%
  rowwise() %>%
  mutate(
    # Apply the function to get modal counts
    modal_counts = list(analyze_modal_functions(text)),
    # Extract individual counts
    world_building = modal_counts["world_building"],
    possibility = modal_counts["possibility"],
    hypothetical = modal_counts["hypothetical"],
    intent = modal_counts["intent"],
    # Get total word count for normalization
    total_words = str_count(text, "\\S+"),
    # Normalize per 1000 words
    world_building_per_k = (world_building / total_words) * 1000,
    possibility_per_k = (possibility / total_words) * 1000,
    hypothetical_per_k = (hypothetical / total_words) * 1000,
    intent_per_k = (intent / total_words) * 1000,
    # Calculate authority ratio (world_building + intent vs. possibility + hypothetical)
    authority_ratio = (world_building + intent) / (possibility + hypothetical)
  ) %>%
  # Select only needed columns
  select(book_id, 
         world_building, possibility, hypothetical, intent, 
         world_building_per_k, possibility_per_k, hypothetical_per_k, intent_per_k,
         authority_ratio)

```

```{r}
#| label: modal_verb_vis
# Get book titles for visualization
modal_results <- modal_results %>%
  left_join(
    gaiman_corpus$document_level %>% 
      select(book_id, title) %>% 
      distinct(),
    by = "book_id"
  )

# Prepare data for visualization
modal_long <- modal_results %>%
  select(book_id, title, ends_with("_per_k")) %>%
  pivot_longer(
    cols = ends_with("_per_k"),
    names_to = "modal_type",
    values_to = "frequency_per_k"
  ) %>%
  # Clean up category names
  mutate(
    modal_type = case_when(
      modal_type == "world_building_per_k" ~ "World Building",
      modal_type == "possibility_per_k" ~ "Possibility",
      modal_type == "hypothetical_per_k" ~ "Hypothetical",
      modal_type == "intent_per_k" ~ "Intent"
    )
  )

# Create improved modal verb usage visualization
modal_plot <- ggplot(modal_long,
                    aes(x = reorder(book_id, frequency_per_k, sum),
                        y = frequency_per_k,
                        fill = modal_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("World Building" = "#5C6BC0",
                               "Possibility" = "#26A69A",
                               "Hypothetical" = "#AB47BC",
                               "Intent" = "#EF5350")) +
  labs(
    title = "Modal Verb Usage by Narrative Function",
    subtitle = "Frequency per 1,000 words across Gaiman's works",
    x = "Book",
    y = "Frequency (per 1,000 words)",
    fill = "Modal Function"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(modal_plot)


```

```{r}
#| label: authority_vis
# Authority ratio visualization
authority_plot <- ggplot(modal_results,
                        aes(x = reorder(book_id, authority_ratio),
                            y = authority_ratio)) +
  geom_col(fill = "#FF7043") +
  geom_text(aes(label = sprintf("%.2f", authority_ratio)), 
            vjust = -0.5, size = 3) +
  labs(
    title = "Narrative Voice Authority in Gaiman's Works",
    subtitle = "Ratio of authoritative modals (world-building + intent) to uncertain modals (possibility + hypothetical)",
    x = "Book",
    y = "Authority Ratio"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display visualizations

print(authority_plot)
```

#### Integrated Analysis 

Consider use PCA to visualise the distribution of the ten books on the spectrum of children-adults.\
Or if you have some better idea.

```{r}
#| label: pca_feature_selection

# PCA Analysis of Linguistic Features in Neil Gaiman's Works
# Purpose: Data-driven exploration of stylistic patterns across books


# Step 1: Compile the features matrix
# -----------------------------------------------------------------
# Extract key metrics from previous analyses into a single dataframe
gaiman_features <- data.frame(
  book_id = gaiman_corpus$metadata$book_level$book_id,
  
  # Lexical features
  lexical_diversity = gaiman_corpus$metadata$book_level$lexical_diversity,
  avg_word_length = word_lengh_stats$avg_length,
  short_word_pct = word_lengh_stats$short_word_pct,
  long_word_pct = word_lengh_stats$long_word_pct,
  
  # Sentence features
  avg_sentence_length = sentence_length_stats$avg_length,
  short_sent_pct = sentence_length_stats$short_sent_pct,
  long_sent_pct = sentence_length_stats$long_sent_pct,
  mean_dep_length = book_dependency_stats$mean_dep_length,
  readability_score = readability_data$Flesch.Kincaid,
  
  # Sentiment and profanity
  positive_sentiment = book_sentiment_distribution$positive_pct,
  negative_sentiment = book_sentiment_distribution$negative_pct,
  total_profanity = profanity_categories$total_per_1000,
  strong_profanity = profanity_categories$strong_per_1000,
  
  # Discourse features
  first_person_pct = pov_stability$pct_first,
  third_person_pct = pov_stability$pct_third,
  pov_stability = pov_stability$stability_index,
  authority_ratio = modal_results$authority_ratio
)

# Step 2: Set book_id as rownames and remove from data matrix
row.names(gaiman_features) <- gaiman_features$book_id
gaiman_features <- gaiman_features %>% select(-book_id)

# Step 3: Run the PCA
# -----------------------------------------------------------------
# scale=TRUE ensures all features are standardized before PCA
gaiman_pca <- prcomp(gaiman_features, scale = TRUE)

# Step 4: Examine basic PCA results
# -----------------------------------------------------------------
# Variance explained by each principal component
summary(gaiman_pca)

# Extract the feature loadings on principal components
loadings <- gaiman_pca$rotation
print(loadings[, 1:2])  # Loadings on first two components

# Step 5: Create a clear, informative visualization
# -----------------------------------------------------------------
# Basic biplot of books and features
pca_biplot <- fviz_pca_biplot(gaiman_pca,
                    # Data points (books)
                    repel = TRUE,  # Avoid text overlap
                    label = "var",  # Show variable names
                    col.var = "darkblue",  # Variable color
                    col.ind = "darkred",   # Individual (book) color
                    # Customization for clarity
                    title = "PCA: Linguistic Features of Gaiman's Works",
                    subtitle = "Books positioned by linguistic similarities",
                    ggtheme = theme_minimal()) +
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "bottom"
  )

print(pca_biplot)

# Step 6: Identify key differentiating features
# -----------------------------------------------------------------
# Calculate feature contribution to first two principal components
feature_contrib <- data.frame(
  feature = colnames(gaiman_features),
  PC1_contribution = abs(loadings[, 1]),
  PC2_contribution = abs(loadings[, 2])
) %>%
  arrange(desc(PC1_contribution))

print(feature_contrib)

```

### Discussion 

### Conclusion 

### Reference

