---
title: "Stylistic Variations in Neil Gaiman's Work: A Corpus-Driven Analysis"
---

## Introduction

As a prominent contemporary author, Neil Gaiman has produced a substantial body of work spanning diverse genres and targeting audiences ranging from children to adults. This project employs corpus-driven methods to analyse potential stylistic variations in Gaiman's writing that may distinguish works intended for different reader demographics. Through systematic linguistic analysis, the study aims to identify and characterize textual features that differentiate Gaiman's writing across audience-targeted works.

```{r}
#| label: packages_loading
#| message: false
#| warning: false
#| include: false
#| paged-print: false
# Load required packages
library(tidyverse)      
library(here)           
library(quanteda)       
library(quanteda.textstats)  
library(sentimentr)     
library(wordcloud)      
library(ggplot2)        
library(gridExtra)     
library(scales)        
library(ggridges)      
library(dplyr)
library(stopwords)
library(udpipe)
library(ggrepel) 
library(factoextra) 
```

```{r}
#| label: corpus_loading

# Read the corpus
gaiman_corpus <- readRDS(here("corpora", "gaiman_corpus_complete.rds"))

# Explore the structure
str(gaiman_corpus, max.level = 1)
```

The corpus contains three levels of the texts: document-level (tokenised by chapter); sentence-level (tokenised by sentence-end punctuation); and token-level (tokenised by word).

(I will put a table here showcasing the sizes of each book with basic statistics)

\
To accelerate the calculation and analysis, some basic statistical features are calculated beforehand and added to the corpus as the layers of Metadata and Vocabulary.

## Literature Review 

-   Previous corpus-based studies on stylistic variation in fiction

-   Research on linguistic markers of audience targeting (children's vs. adult literature)

-   Studies on Neil Gaiman's writing style (if available)

-   **Research Questions**:

    1.  What lexical features differentiate Gaiman's writing across audience demographics?

    2.  How do syntactic complexity and sentence-level features vary across Gaiman's works?

    3.  What discourse-level patterns (POV, modality) characterize works for different audiences?

    4.  How do these features collectively distinguish his works for different reader demographics?

## Methodology 

### Corpus 

-   **Data**: Description of the 10 books with their basic information (provide a table with titles, word counts, publication dates, and target audience)

-   **Structure**: Explanation of your three-tiered corpus design (document, sentence, token levels)

-   Description of metadata and vocabulary components

-   Preprocessing steps

### Analysis 

#### lexical level 

```{r}
#| label: exploratory_analysis
# Extract book-level metadata
book_metadata <- gaiman_corpus$metadata$book_level

# Display basic statistics about the corpus 
corpus_summary <- book_metadata |> 
  summarise(
    total_books = n(),
    total_words = sum(total_words),
    avg_words_per_book = mean(total_words),
    avg_chapters = mean(chapters),
    avg_sentences = mean(sentences),
    avg_sentence_length = mean(avg_sentence_length),
    avg_lexical_diversity = mean(lexical_diversity)
  )

print(corpus_summary)
```

#### 1. Vocabulary Complexity

##### Lexical Diversity

First, we check lexical diversity

```{r}
#| label: ttr_visualisation
# Lexical diversity is already stored in the corpus, so we just need to visulaise it 
lex_div_plot <- ggplot(book_metadata, 
                      aes(x = reorder(book_id, lexical_diversity), 
                          y = lexical_diversity)) +
  geom_bar(stat = "identity", fill = "lightgreen", alpha = 0.8) +
  # Add reference line for average
  geom_hline(yintercept = mean(book_metadata$lexical_diversity), 
             linetype = "dashed", color = "red") +
  # Add text annotation for average
  annotate("text", 
           x = 1, 
           y = mean(book_metadata$lexical_diversity) + 0.01, 
           label = "Corpus Average", 
           hjust = 0, 
           color = "red") +
  labs(
    title = "Lexical Diversity by Book",
    subtitle = "Higher values indicate more varied vocabulary",
    x = "Book",
    y = "Type-Token Ratio"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(lex_div_plot)

```

It might be counter-intuitive to see that the shortest, very children-oriented painted book Fortunately, the Milk has the highest TTR, while the longer, adult-targeted books such as American Gods and Anasi Boys are the lowerest. However, it makes sense. Given that FTM is the shortest, the words are more likely to appear in fewer times, even only once, which increases the ratio of type. Being opposite to that, the long novels contain much more words of which many are common words. They water down TTR. This calls for more detailed and comprehensive stylistic analysis on multiple layers of the text.

##### Word Length Distribution

Moving on, we check the distribution of word length

```{r}
#| label: word_length_cal
#| message: false
#| warning: false
#| paged-print: false
# Calculate word length for all tokens
token_length <- gaiman_corpus$token_level |> 
  # Calculate character length of each token 
  mutate(word_length = nchar(token))

# With token length, we can calculate word lengh by book
word_lengh_stats <- token_length |> 
  group_by(book_id) |> 
  summarise(
    avg_length = mean(word_length),
    median_length = median(word_length),
    sd_length = sd(word_length),
    # We can further check the percentage of words by length 
    short_word_pct = mean(word_length <= 4) * 100, # 1-4 words = short
    medium_word_pct = mean(word_length > 4 & word_length <= 8)* 100, # 4 - 8 = medium
    long_word_pct = mean(word_length >8)*100, # 9+ words are long
    .groups = "drop"
  ) |> 
  arrange(desc(avg_length))

print(word_lengh_stats)
```

We can visualise the results

```{r}
#| label: word_length_vis
word_lengh_dist <- token_length |> 
  # Count occurrences of each word length in each book 
  count(book_id, word_length) |> 
  # Group by book to calculate percentages 
  group_by(book_id) |> 
  mutate(percentage = n / sum(n) * 100) |> 
  ungroup() |> 
  # start vis
  ggplot(aes(x = word_length, 
             y = percentage,
             color = book_id,
             group = book_id)) +
  geom_line(linewidth = 1, alpha = 0.7) + # no more "size", update to linewidth
  labs(
    title = "Word Length Distribution by Book", 
    subtitle = "Percentage of words at each character length",
    x = "Word Length (by character)",
    Y = "Percentage of books",
    color = "Book"
    
  )+
  theme_classic() +
  theme(legend.position = "bottom")

print(word_lengh_dist)
```

As the chart shows, Gaiman's writing shows a consistent pattern across his works. Most words are at around 4 characters, which reflect the common pattern of English.

##### Word Frequency

We first remove common stopwords before checking the word frequency

```{r}
#| label: word_frequnecy
vocabulary <- gaiman_corpus$vocabulary
stopwords = stopwords(language = "en")

top_content_words <- vocabulary |> 
  filter(!(token_lower %in% stopwords)) |> 
  arrange(desc(frequency)) |> 
  head(30)

# Skip printing resutls, directly visualise the bar chart
top_words_plot <- ggplot(top_content_words,
                         aes(x = reorder(token_lower, frequency),
                             y = frequency)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  labs(
    title = "Top 30 Conent Words in Gaiman Corpus",
    subtitle = "Stopwords applied",
    x = "Word",
    y = "Frequency"
  ) + 
  theme_classic() +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))

print(top_words_plot)
```

#### 2. Lexical Choices

We will use the package `quanteda` for most lexical analysis

First, we will prepare quanteda corpora for analysis:

```{r}
#| label: quanteda_loading
# Create a corpus from the document level data
quanteda_corpus <- corpus(
  gaiman_corpus$document_level$text,
  docnames = gaiman_corpus$document_level$doc_id
)

# Add document variables for grouping
docvars(quanteda_corpus, "book_id") <- gaiman_corpus$document_level$book_id
docvars(quanteda_corpus, "title") <- gaiman_corpus$document_level$title
docvars(quanteda_corpus, "chapter_num") <- gaiman_corpus$document_level$chapter_num

```

-   Keyword analysis

    -   consider a big table showing the top 10 keywords of the ten books

        -   Maybe not too much, because that would be too dense to read

            -   if there are shared keywords, consider showing the different collocations

##### Keyword analysis

We use log-off to calculate keywords of the ten books

```{r}
#| label: log_off_cal
# Log odds function to compare one book against all others
# This identifies distinctive words in each book compared to other books
calculate_log_odds <- function(dfm_obj, target_book) {
  # Get index of target book
  target_idx <- which(docnames(dfm_obj) == target_book)
  
  # Skip if book not found
  if(length(target_idx) == 0) return(NULL)
  
  # Get counts for target book
  target_counts <- as.numeric(dfm_obj[target_idx,])
  target_total <- sum(target_counts)
  
  # Get counts for all other books
  others_counts <- colSums(dfm_obj[-target_idx,])
  others_total <- sum(others_counts)
  
  # Calculate log odds (adding small constant to prevent division by zero)
  epsilon <- 0.5  # Smoothing constant
  log_odds <- log((target_counts + epsilon) / (target_total - target_counts + epsilon)) - 
              log((others_counts + epsilon) / (others_total - others_counts + epsilon))
  
  # Create results data frame
  data.frame(
    term = colnames(dfm_obj),
    log_odds = log_odds,
    target_count = target_counts,
    others_count = others_counts,
    stringsAsFactors = FALSE
  ) %>%
    # Sort by log odds (descending)
    arrange(desc(log_odds))
}

# Create document-feature matrix
book_dfm <- tokens(quanteda_corpus) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_remove(pattern = "[\\d\\p{P}]", valuetype = "regex") %>% # Remove numbers and punctuation
  dfm() %>%
  dfm_trim(min_termfreq = 3) %>%  # Remove very rare terms
  dfm_group(groups = docvars(quanteda_corpus, "book_id"))

# Get log odds for each book
book_ids <- unique(docvars(quanteda_corpus, "book_id"))
all_keywords <- list()

for (book_id in book_ids) {
  # Calculate log odds
  book_keywords <- calculate_log_odds(book_dfm, book_id)
  
  # Skip if book not found in DFM
  if(is.null(book_keywords)) next
  
  # Keep top 20 distinctive words
  book_keywords <- book_keywords %>%
    head(20) %>%
    mutate(book_id = book_id)
  
  # Add to results list
  all_keywords[[book_id]] <- book_keywords
}

# Combine results
keywords_df <- bind_rows(all_keywords)
```

We can visualise the results:

```{r}
#| label: log_off_vis
# Visualize top 10 keywords for each book
top_keywords_plot <- keywords_df %>%
  group_by(book_id) %>%
  slice_head(n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = tidytext::reorder_within(term, log_odds, book_id), 
             y = log_odds, 
             fill = book_id)) +
  geom_col() +
  facet_wrap(~ book_id, scales = "free_y") +
  upstartr::scale_x_reordered() +  # Required for reorder_within
  coord_flip() +
  labs(
    title = "Top 10 Distinctive Words by Book",
    subtitle = "Based on log odds ratio compared to other books",
    x = "Term",
    y = "Log Odds Ratio"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(top_keywords_plot)
```

#### Sentence Level

#### 1. Syntactic Structures

-   Sentence length variation

```{r}
#| label: sent_loading

# Prepare sentence data
sentence_data <- gaiman_corpus$sentence_level %>%
  select(book_id, title, chapter_num, sentence_id, text)
```

```{r}
#| label: sentent_length_dist_vis
# Calculate sentence length statistics
sentence_length_stats <- sentence_data %>%
  # Add sentence length in words
  mutate(sentence_length = sapply(strsplit(text, "\\s+"), length)) %>%
  # Group by book
  group_by(book_id) %>%
  # Calculate statistics
  summarise(
    avg_length = mean(sentence_length),
    median_length = median(sentence_length),
    sd_length = sd(sentence_length),
    min_length = min(sentence_length),
    max_length = max(sentence_length),
    short_sent_pct = mean(sentence_length <= 5) * 100,  # Percentage of short sentences
    long_sent_pct = mean(sentence_length > 20) * 100    # Percentage of long sentences
  ) %>%
  arrange(desc(avg_length))

# Visualize sentence length distribution
sentence_length_vis <- sentence_data %>%
  # Add sentence length in words
  mutate(sentence_length = sapply(strsplit(text, "\\s+"), length)) %>%
  # Remove extreme outliers for better visualization
  filter(sentence_length <= 50) %>%
  ggplot(aes(x = sentence_length, y = book_id, fill = book_id)) +
  geom_density_ridges(alpha = 0.7, scale = 3) +
  labs(
    title = "Sentence Length Distribution by Book",
    subtitle = "Ridgeline plot of word counts per sentence",
    x = "Words per Sentence",
    y = "Book"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

print(sentence_length_vis)
```

-   syntactic complexity

Mean Dependency Length Analysis: the average distance between words and their syntactic heads

```{r}
#| label: ud_cal


# 1. Function to calculate mean dependency length for parsed text
calculate_mean_dep_length <- function(parsed_data) {
  # For each sentence, calculate dependency lengths
  parsed_data %>%
    # Filter out root nodes and punctuation
    filter(head_token_id != 0, upos != "PUNCT") %>%
    # Calculate absolute distance between each word and its head
    mutate(
      # Convert IDs to numeric and calculate distance
      token_id_num = as.numeric(token_id),
      head_id_num = as.numeric(head_token_id),
      # Absolute difference is the dependency length
      dep_length = abs(token_id_num - head_id_num)
    ) %>%
    # Calculate mean dependency length
    summarize(
      mean_dep_length = mean(dep_length, na.rm = TRUE),
      median_dep_length = median(dep_length, na.rm = TRUE),
      max_dep_length = max(dep_length, na.rm = TRUE),
      deps_analyzed = n(),
      .groups = "drop"
    )
}

# 2. Process a sample of text from each book to calculate dependency lengths
# This avoids processing the entire corpus which could be time-consuming
set.seed(42) # For reproducibility in sampling
sample_size <- 100 # Sample 100 sentences per book for analysis

# Get sample sentences from each book - FIXED version
sampled_sentences <- sentence_data %>%
  # Split by book_id
  group_by(book_id) %>%
  # Use group_modify to properly access group size
  group_modify(~{
    # Get the minimum of sample_size or actual group size
    sample_size_to_use <- min(sample_size, nrow(.x))
    # Sample that many rows
    slice_sample(.x, n = sample_size_to_use)
  }) %>%
  ungroup()

# Create a unique identifier for each sampled sentence
sampled_sentences <- sampled_sentences %>%
  mutate(sample_id = paste0(book_id, "_", row_number()))

# Initialize dependency parsing model
# Note: This requires downloading a model - this block shows expected code flow
dep_model <- udpipe_download_model(language = "english-ewt", model_dir = getwd())
ud_model <- udpipe_load_model(dep_model$file_model)

# Process the sample sentences with udpipe for dependency parsing
# Using the sample_id as doc_id to maintain uniqueness
parsed_samples <- udpipe(
  x = sampled_sentences$text, 
  object = ud_model,
  doc_id = sampled_sentences$sample_id,  # Use our unique sample_id
  parallel.cores = 1
)

# Create a mapping between sample_id and book_id
sample_book_mapping <- sampled_sentences %>%
  select(sample_id, book_id, title) %>%
  distinct()

# 3. Calculate mean dependency length for each sentence, then aggregate by book
sentence_dependency_stats <- parsed_samples %>%
  # Group by doc_id (which is our sample_id)
  group_by(doc_id) %>%
  # Calculate dependency stats for each sentence
  group_modify(~calculate_mean_dep_length(.x))

# Join with our mapping to get book information
book_dependency_stats <- sentence_dependency_stats %>%
  # Join with mapping to get book_id
  left_join(sample_book_mapping, by = c("doc_id" = "sample_id")) %>%
  # Group by book to aggregate sentence-level stats
  group_by(book_id, title) %>%
  # Calculate book-level averages
  summarize(
    mean_dep_length = mean(mean_dep_length, na.rm = TRUE),
    median_dep_length = mean(median_dep_length, na.rm = TRUE),
    max_dep_length = max(max_dep_length, na.rm = TRUE),
    sentences_analyzed = n(),
    .groups = "drop"
  )

```

```{r}
#| label: ud_vis

# 4. Create visualization of mean dependency length
dep_length_plot <- ggplot(book_dependency_stats,
  # Sort by mean dependency length
  aes(x = reorder(book_id, mean_dep_length),
      y = mean_dep_length)) +
  geom_col(fill = "#5D93E1") +
  # Add error bars showing variability
  geom_errorbar(
    aes(ymin = mean_dep_length - median_dep_length/4, 
        ymax = mean_dep_length + median_dep_length/4),
    width = 0.2, color = "#2C528C"
  ) +
  labs(
    title = "Syntactic Complexity in Gaiman's Works",
    subtitle = "Mean dependency length (average distance between related words)",
    x = "Book",
    y = "Mean Dependency Length"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )

# 5. Find illustrative examples of short and long dependencies
# We'll find examples from the original parsed sentences
parsed_with_dep_length <- parsed_samples %>%
  # Calculate dependency length for each token
  mutate(
    token_id_num = as.numeric(token_id),
    head_id_num = as.numeric(head_token_id),
    dep_length = abs(token_id_num - head_id_num)
  )

# Aggregate to get max dependency length in each sentence
sentence_max_deps <- parsed_with_dep_length %>%
  group_by(doc_id, sentence_id) %>%
  summarize(
    max_dep = max(dep_length[upos != "PUNCT" & head_token_id != "0"], na.rm = TRUE),
    sentence_text = first(sentence),
    .groups = "drop"
  ) %>%
  # Remove potential NA/Inf values from empty sentences
  filter(is.finite(max_dep))

# Find better examples
short_example <- sentence_max_deps %>%
  filter(max_dep > 1 & max_dep <= 3) %>%  # Not too trivial, but still simple
  filter(nchar(sentence_text) > 10) %>%    # Ensure it's a real sentence
  arrange(max_dep) %>%
  slice(1)

long_example <- sentence_max_deps %>%
  filter(max_dep >= 7) %>%                # Clearly complex
  arrange(desc(max_dep)) %>%
  slice(1)

# Join with book information for examples
example_sentences <- bind_rows(
  short_example %>% mutate(type = "Short dependency"),
  long_example %>% mutate(type = "Long dependency")
) %>%
  left_join(sample_book_mapping, by = c("doc_id" = "sample_id"))

# Print results
print(dep_length_plot)

# Print example sentences with book sources
cat("\nExample sentences illustrating dependency length:\n\n")

for(i in 1:nrow(example_sentences)) {
  cat(paste0(example_sentences$type[i], " (max = ", 
             round(example_sentences$max_dep[i], 1), 
             ") from ", example_sentences$title[i], ":\n"))
  cat(example_sentences$sentence_text[i], "\n\n")
}
```

#### Sentiment Analysis

```{r}
#| label: senti_ana_vis

# Load required libraries
library(sentimentr)
library(dplyr)
library(tidyr)
library(ggplot2)

# First, let's set up a simple analysis pipeline

# 1. Calculate sentence-level sentiment
sentence_sentiment <- sentiment_by(
  text.var = sentence_data$text,
  by = list(
    book_id = sentence_data$book_id,
    sentence_id = sentence_data$sentence_id
  )
)

# 2. Classify each sentence as positive, negative, or neutral
sentence_sentiment <- sentence_sentiment %>%
  mutate(
    sentiment_class = case_when(
      ave_sentiment > 0.05 ~ "Positive",
      ave_sentiment < -0.05 ~ "Negative",
      TRUE ~ "Neutral"  # Use a small buffer around zero for neutral
    )
  )

# 3. Calculate the distribution for each book
book_sentiment_distribution <- sentence_sentiment %>%
  group_by(book_id) %>%
  summarize(
    total_sentences = n(),
    positive_count = sum(sentiment_class == "Positive"),
    negative_count = sum(sentiment_class == "Negative"),
    neutral_count = sum(sentiment_class == "Neutral"),
    positive_pct = (positive_count / total_sentences) * 100,
    negative_pct = (negative_count / total_sentences) * 100,
    neutral_pct = (neutral_count / total_sentences) * 100
  )

# 4. Verify our percentages add up to 100%
book_sentiment_distribution <- book_sentiment_distribution %>%
  mutate(total_pct = positive_pct + negative_pct + neutral_pct)

# Check results (should be close to 100% for each book)
print(book_sentiment_distribution[, c("book_id", "total_pct")])

# 5. Create data for visualization
sentiment_long <- book_sentiment_distribution %>%
  select(book_id, positive_pct, negative_pct, neutral_pct) %>%
  pivot_longer(
    cols = c("positive_pct", "negative_pct", "neutral_pct"),
    names_to = "sentiment",
    values_to = "percentage"
  ) %>%
  mutate(
    sentiment = case_when(
      sentiment == "positive_pct" ~ "Positive",
      sentiment == "negative_pct" ~ "Negative",
      sentiment == "neutral_pct" ~ "Neutral"
    )
  )

# 6. Create a simple stacked bar chart
sentiment_distribution_plot <- ggplot(sentiment_long, 
                                     aes(x = book_id, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(values = c(
    "Positive" = "#4CAF50",
    "Negative" = "#F44336",
    "Neutral" = "#9E9E9E"
  )) +
  labs(
    title = "Sentiment Distribution in Gaiman's Works",
    subtitle = "Percentage of positive, negative, and neutral sentences in each book",
    x = "Book",
    y = "Percentage of Sentences",
    fill = "Sentiment"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  # Ensure y-axis goes from 0-100%
  scale_y_continuous(limits = c(0, 100))

# Display the plot
print(sentiment_distribution_plot)

# Optional: Generate a summary table for reporting
sentiment_summary <- book_sentiment_distribution %>%
  select(book_id, positive_pct, negative_pct, neutral_pct) %>%
  arrange(desc(positive_pct))

print(sentiment_summary)
```

```{r}
# Normalized Sentiment Flow Analysis
# ---------------------------------------
# This function creates a normalized sentiment flow chart for all books
# showing how sentiment changes throughout the narrative progression

# 1. Calculate normalized chapter positions and sentiment scores for all books
normalized_sentiment_flow <- sentence_sentiment %>%
  # Group by book and chapter to get average sentiment per chapter
  group_by(book_id, chapter_num) %>%
  summarise(
    chapter_sentiment = mean(ave_sentiment, na.rm = TRUE),
    samples = n(),  # Track number of sentences for potential weighting
    .groups = "drop"
  ) %>%
  # For each book, calculate normalized position (0-100%)
  group_by(book_id) %>%
  mutate(
    # Get the max chapter number for each book
    max_chapter = max(chapter_num),
    # Calculate normalized position (0-100% of narrative)
    narrative_position = (chapter_num / max_chapter) * 100,
    # Calculate z-scores for better cross-book comparison
    sentiment_z = scale(chapter_sentiment)[,1]
  ) %>%
  # Get book titles for better labeling
  left_join(
    distinct(sentence_sentiment[, c("book_id", "title")]),
    by = "book_id"
  )

# Fix missing data for ST and TGB
# First check what's happening with these books' data
missing_books_data <- normalized_sentiment_flow %>%
  filter(book_id %in% c("ST", "TGB")) %>%
  summarise(
    total_rows = n(),
    na_count = sum(is.na(chapter_sentiment)),
    zero_count = sum(chapter_sentiment == 0, na.rm = TRUE),
    min_val = min(chapter_sentiment, na.rm = TRUE),
    max_val = max(chapter_sentiment, na.rm = TRUE)
  )
print(missing_books_data)

# Fix 1: Replace NA values with 0 (if that's the issue)
normalized_sentiment_flow <- normalized_sentiment_flow %>%
  mutate(chapter_sentiment = ifelse(is.na(chapter_sentiment), 0, chapter_sentiment))

# 2. Create the normalized sentiment flow visualization for all books
narrative_flow_plot <- ggplot(normalized_sentiment_flow, 
                             aes(x = narrative_position, 
                                 y = chapter_sentiment, 
                                 color = book_id)) +
  # Add smoothed trend lines to see the emotional arc
  geom_smooth(se = FALSE, span = 0.5, size = 1) +
  # Add small points for actual chapter sentiments
  geom_point(alpha = 0.4, size = 1.5) +
  # Facet by book for easier comparison, use a small multiple approach
  facet_wrap(~ title, scales = "free_y") +
  # Add clear labels
  labs(
    title = "Emotional Arcs in Neil Gaiman's Works",
    subtitle = "Sentiment progression throughout narrative (0% = beginning, 100% = end)",
    x = "Narrative Progression (%)",
    y = "Sentiment Score",
    color = "Book"
  ) +
  # Use a subdued color scheme that works for multiple lines
  scale_color_brewer(palette = "Set2") +
  # Remove legend since we're using facets
  theme_minimal() +
  theme(
    legend.position = "none",
    panel.spacing = unit(1, "lines"),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# 3. Create a single-panel version showing all books for direct comparison
combined_flow_plot <- ggplot(normalized_sentiment_flow, 
                            aes(x = narrative_position, 
                                y = chapter_sentiment, 
                                color = book_id)) +
  # Add smoothed trend lines
  geom_smooth(se = FALSE, span = 0.5, size = 1.2) +
  # Add book-specific endpoints to see where each narrative concludes
  geom_point(data = normalized_sentiment_flow %>% 
               group_by(book_id) %>% 
               filter(narrative_position == max(narrative_position)),
             size = 3, shape = 18) +
  # Add clear labels
  labs(
    title = "Comparing Emotional Arcs Across Gaiman's Works",
    subtitle = "How sentiment evolves from beginning (0%) to end (100%) of each book",
    x = "Narrative Progression (%)",
    y = "Average Sentiment Score",
    color = "Book"
  ) +
  # Add a horizontal reference line at neutral sentiment (0)
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", size = 0.5) +
  # Use a color palette that distinguishes between books
  scale_color_brewer(palette = "Paired") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank()
  )

# To make it focused, only show the combined flow 
print(combined_flow_plot)

```

```{r}
# 1. First check if ST and TGB exist in the original corpus
book_presence <- gaiman_corpus$document_level %>%
  group_by(book_id) %>%
  summarise(
    chapters = n_distinct(chapter_num),
    total_text = sum(nchar(text))
  )
print(book_presence)

# 2. Check for chapter structure issues
chapter_structure <- gaiman_corpus$document_level %>%
  filter(book_id %in% c("ST", "TGB")) %>%
  group_by(book_id) %>%
  summarise(
    min_chapter = min(chapter_num),
    max_chapter = max(chapter_num),
    chapter_count = n_distinct(chapter_num),
    sequential_chapters = max_chapter - min_chapter + 1 == chapter_count
  )
print(chapter_structure)

# 3. Examine sentiment calculation at sentence level for these books
sentiment_check <- sentence_sentiment %>%
  filter(book_id %in% c("ST", "TGB")) %>%
  group_by(book_id, chapter_num) %>%
  summarise(
    sentence_count = n(),
    na_sentiment_count = sum(is.na(ave_sentiment)),
    avg_sentiment = mean(ave_sentiment, na.rm = TRUE),
    .groups = "drop"
  )
print(sentiment_check)

# 4. Look for anomalies in the chapter aggregation
problem_chapters <- normalized_sentiment_flow %>%
  filter(book_id %in% c("ST", "TGB")) %>%
  select(book_id, chapter_num, chapter_sentiment, narrative_position, sentiment_z) %>%
  arrange(book_id, chapter_num)
print(problem_chapters)
```

-   

-   \
    Prohibitive Expressions Analysis

**swear analysis with examples**

```{r}
#| label: swear_analysis_calculation


# 2. Define categorized profanity dictionaries
# Expanded lists with more comprehensive coverage
mild_profanity <- c(
  "damn", "darn", "hell", "crap", "suck", "stupid", "idiot", "dumb", 
  "moron", "fool", "jerk", "dork", "heck", "gosh", "jeez", "Christ", 
  "God", "Jesus", "bloody", "blast", "drat", "freaking", "frigging",
  "poop", "butt", "arse", "bollocks", "turd", "bum"
)

moderate_profanity <- c(
  "ass", "asshole", "bastard", "shit", "bullshit", "piss", 
  "screw", "screwed", "dick", "cock", "prick", "slut", "whore", 
  "wanker", "tosser", "twat", "crap", "balls", "nuts", "pissed", 
  "fart", "jackass", "douche", "douchebag", "tits", "boobs"
)

strong_profanity <- c(
  "fuck", "fucked", "fucker", "fucking", "motherfucker", "motherfucking",
  "cunt", "pussy", "cock", "bitch", "cum", "jizz", "nigger", "faggot", 
  "fag", "spic", "chink", "kike", "retard", "whore", "slut"
)

# Combine all for overall profanity detection
all_profanity <- unique(c(mild_profanity, moderate_profanity, strong_profanity))

# 3. Prepare sentence-level data
sentence_data <- gaiman_corpus$sentence_level %>%
  select(book_id, title, sentence_id, text)

# 4. Apply overall profanity detection 
profanity_results <- profanity_by(
  text.var = sentence_data$text,
  by = sentence_data$book_id,
  profanity_list = all_profanity
)

# 5. Create function to count profanity by category
count_profanity_by_category <- function(book_id, sentence_data) {
  # Extract text for this book
  book_text <- sentence_data %>%
    filter(book_id == !!book_id) %>%
    pull(text) %>%
    paste(collapse = " ") %>%
    tolower()
  
  # Count instances of each category
  mild_count <- sum(sapply(mild_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  moderate_count <- sum(sapply(moderate_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  strong_count <- sum(sapply(strong_profanity, function(term) {
    str_count(book_text, paste0("\\b", term, "\\b"))
  }))
  
  # Return counts by category
  return(c(
    mild = mild_count,
    moderate = moderate_count,
    strong = strong_count,
    total = mild_count + moderate_count + strong_count
  ))
}

# 6. Apply category counting to each book
book_ids <- unique(sentence_data$book_id)
category_counts <- lapply(book_ids, function(id) {
  counts <- count_profanity_by_category(id, sentence_data)
  data.frame(
    book_id = id,
    mild_count = counts["mild"],
    moderate_count = counts["moderate"],
    strong_count = counts["strong"],
    total_count = counts["total"]
  )
})

# Combine results
profanity_categories <- bind_rows(category_counts) %>%
  # Join with word counts from profanity_results
  left_join(
    profanity_results %>% select(book_id, word_count),
    by = "book_id"
  ) %>%
  # Calculate rates per 1000 words
  mutate(
    mild_per_1000 = (mild_count / word_count) * 1000,
    moderate_per_1000 = (moderate_count / word_count) * 1000,
    strong_per_1000 = (strong_count / word_count) * 1000,
    total_per_1000 = (total_count / word_count) * 1000
  ) %>%
  # Sort by total profanity rate
  arrange(desc(total_per_1000))

# 7. Create visualization of profanity categories
# Prepare data for stacked bar chart
profanity_long <- profanity_categories %>%
  select(book_id, ends_with("per_1000")) %>%
  # Keep only category columns
  select(-total_per_1000) %>%
  # Convert to long format
  pivot_longer(
    cols = ends_with("per_1000"),
    names_to = "category",
    values_to = "rate_per_1000"
  ) %>%
  # Clean up category names
  mutate(
    category = case_when(
      category == "mild_per_1000" ~ "Mild",
      category == "moderate_per_1000" ~ "Moderate", 
      category == "strong_per_1000" ~ "Strong"
    ),
    # Convert to factor with desired order
    category = factor(category, levels = c("Mild", "Moderate", "Strong"))
  )

# Create stacked bar chart
category_plot <- ggplot(profanity_long, 
                       aes(x = reorder(book_id, rate_per_1000, sum), 
                           y = rate_per_1000,
                           fill = category)) +
  geom_col() +
  scale_fill_manual(values = c(
    "Mild" = "#FFC107",
    "Moderate" = "#FF9800", 
    "Strong" = "#F44336"
  )) +
  labs(
    title = "Profanity in Gaiman's Works by Intensity Level",
    subtitle = "Words per 1,000 categorized by severity",
    x = "Book",
    y = "Frequency (per 1,000 words)",
    fill = "Profanity Level"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )

# 8. Function to extract examples for each category
extract_category_examples <- function(book_id, category, word_list, n_examples = 2) {
  # Get sentences for this book
  book_sentences <- sentence_data %>%
    filter(book_id == !!book_id)
  
  # Find sentences containing words from the specified category
  matches <- list()
  for (term in word_list) {
    pattern <- paste0("\\b", term, "\\b")
    for (i in 1:nrow(book_sentences)) {
      text <- book_sentences$text[i]
      if (str_detect(tolower(text), pattern)) {
        # Create highlighted text with term in asterisks
        highlighted <- str_replace_all(
          text, 
          regex(pattern, ignore_case = TRUE),
          paste0("**", term, "**")
        )
        matches <- c(matches, list(list(
          sentence_id = book_sentences$sentence_id[i],
          text = highlighted,
          term = term
        )))
        # Stop if we have enough examples
        if (length(matches) >= n_examples) break
      }
    }
    # Stop if we have enough examples
    if (length(matches) >= n_examples) break
  }
  
  # Convert list to data frame
  if (length(matches) > 0) {
    result <- do.call(rbind, lapply(matches, function(x) {
      data.frame(
        sentence_id = x$sentence_id,
        text = x$text,
        term = x$term,
        stringsAsFactors = FALSE
      )
    }))
    return(head(result, n_examples))
  } else {
    return(data.frame(
      sentence_id = integer(0),
      text = character(0),
      term = character(0)
    ))
  }
}

# 9. Extract examples for top books
# Get top 3 books by total profanity
top_books <- profanity_categories %>%
  top_n(3, total_per_1000) %>%
  pull(book_id)

# Extract examples for each category from each top book
profanity_examples <- list()
for (book in top_books) {
  profanity_examples[[book]] <- list(
    mild = extract_category_examples(book, "Mild", mild_profanity),
    moderate = extract_category_examples(book, "Moderate", moderate_profanity),
    strong = extract_category_examples(book, "Strong", strong_profanity)
  )
}


```

```{r}
#| label: swear_analysis_vis


# 10. Display results
print(category_plot)


```

```{r}
#| label: swear_analysis_stats
#| 
# Print category statistics
cat("\nProfanity statistics by category and book:\n")
print(profanity_categories %>% 
      select(book_id, mild_per_1000, moderate_per_1000, strong_per_1000, total_per_1000))

```

```{r}
#| label: swear_analysis_examples
# 11. Print examples with category context
cat("\nExamples of profanity by category from top books:\n\n")
for (book in top_books) {
  cat("===== Examples from", book, "=====\n")
  
  # Print mild examples
  cat("\nMILD PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$mild
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No mild profanity examples found.\n")
  }
  
  # Print moderate examples
  cat("\nMODERATE PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$moderate
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No moderate profanity examples found.\n")
  }
  
  # Print strong examples
  cat("\nSTRONG PROFANITY EXAMPLES:\n")
  examples <- profanity_examples[[book]]$strong
  if (nrow(examples) > 0) {
    for (i in 1:nrow(examples)) {
      cat(i, ": ", examples$text[i], "\n")
    }
  } else {
    cat("No strong profanity examples found.\n")
  }
  
  cat("\n")
}
```

#### 3. Readability Measure

```{r}
# Data-driven Readability Analysis
# ---------------------------------------
# Focus solely on Flesch-Kincaid Grade Level without imposing age categories

# 1. Calculate readability scores for each book
book_texts <- gaiman_corpus$document_level %>%
  group_by(book_id) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup()

# 2. Use quanteda for readability calculations
readability_scores <- textstat_readability(
  book_texts$text,
  measure = c("Flesch.Kincaid")  # Focus on just this measure
)
readability_scores$book_id <- book_texts$book_id

# 3. Join with book information
readability_data <- readability_scores %>%
  # Get book information (title only)
  left_join(
    gaiman_corpus$metadata$book_level %>% 
      select(book_id, title), 
    by = "book_id"
  )

# 4. Create a cleaner, data-driven visualization
readability_plot <- ggplot(readability_data,
  # Sort by Flesch-Kincaid score (reading difficulty)
  aes(x = reorder(book_id, Flesch.Kincaid),
      y = Flesch.Kincaid)) +
  # Use a gradient color reflecting complexity, but not imposing categories
  geom_col(aes(fill = Flesch.Kincaid)) +
  scale_fill_gradient(low = "#E3F2FD", high = "#1565C0") +
  # Add labeled reference lines for grade level interpretation
  geom_hline(yintercept = c(5, 8, 12), linetype = "dashed", color = "gray60") +
  annotate("text", x = 1, y = 5.2, label = "5th Grade", hjust = 0, size = 3, color = "gray40") +
  annotate("text", x = 1, y = 8.2, label = "8th Grade", hjust = 0, size = 3, color = "gray40") +
  annotate("text", x = 1, y = 12.2, label = "12th Grade", hjust = 0, size = 3, color = "gray40") +
  labs(
    title = "Reading Difficulty of Gaiman's Works",
    subtitle = "Flesch-Kincaid Grade Level (higher = more difficult reading)",
    x = "Book",
    y = "Reading Grade Level",
    fill = "Grade Level"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank(),
    legend.position = "right"
  )

# 5. Add book titles to the plot for clearer identification
readability_plot <- readability_plot +
  # Add book titles as text labels
  geom_text(
    aes(label = title, y = 0.5),
    angle = 90, hjust = 0, size = 2.5, color = "white"
  )

# Display the visualization
print(readability_plot)
```

#### Discourse level

#### 1. Narrative Perspective

-   Point of view
    -   pronoun patterns

```{r}
# Function to analyze POV markers in text
analyze_pov <- function(text) {
  # Define POV marker patterns
  first_person <- "\\b(I|me|my|mine|myself|we|us|our|ours|ourselves)\\b"
  second_person <- "\\b(you|your|yours|yourself|yourselves)\\b"
  third_person_m <- "\\b(he|him|his|himself)\\b"
  third_person_f <- "\\b(she|her|hers|herself)\\b"
  third_person_n <- "\\b(it|its|itself)\\b"
  third_person_p <- "\\b(they|them|their|theirs|themselves)\\b"
  
  # Count matches (case insensitive)
  first <- str_count(tolower(text), first_person)
  second <- str_count(tolower(text), second_person)
  third_m <- str_count(tolower(text), third_person_m)
  third_f <- str_count(tolower(text), third_person_f)
  third_n <- str_count(tolower(text), third_person_n)
  third_p <- str_count(tolower(text), third_person_p)
  
  # Calculate totals
  first_total <- first
  second_total <- second
  third_total <- third_m + third_f + third_n + third_p
  
  # Return counts
  return(c(
    first_person = first_total,
    second_person = second_total,
    third_person = third_total,
    third_male = third_m,
    third_female = third_f,
    third_neutral = third_n,
    third_plural = third_p
  ))
}

# Apply to book texts
pov_results <- book_texts %>%
  rowwise() %>%
  mutate(pov_data = list(analyze_pov(text))) %>%
  ungroup() %>%
  mutate(
    first_person = map_dbl(pov_data, ~ .x["first_person"]),
    second_person = map_dbl(pov_data, ~ .x["second_person"]),
    third_person = map_dbl(pov_data, ~ .x["third_person"]),
    third_male = map_dbl(pov_data, ~ .x["third_male"]),
    third_female = map_dbl(pov_data, ~ .x["third_female"]),
    third_neutral = map_dbl(pov_data, ~ .x["third_neutral"]),
    third_plural = map_dbl(pov_data, ~ .x["third_plural"]),
    total_words = str_count(text, "\\S+"),
    # Calculate percentages
    first_pct = first_person / total_words * 100,
    second_pct = second_person / total_words * 100,
    third_pct = third_person / total_words * 100
  ) %>%
  select(-pov_data, -text)

# Visualize POV distribution
pov_long <- pov_results %>%
  select(book_id, first_pct, second_pct, third_pct) %>%
  pivot_longer(
    cols = c(first_pct, second_pct, third_pct),
    names_to = "pov",
    values_to = "percentage"
  ) %>%
  mutate(
    pov = case_when(
      pov == "first_pct" ~ "First Person",
      pov == "second_pct" ~ "Second Person",
      pov == "third_pct" ~ "Third Person"
    )
  )

pov_plot <- ggplot(pov_long, 
                  aes(x = book_id, y = percentage, fill = pov)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Point of View Distribution by Book",
    subtitle = "Percentage of personal pronouns by category",
    x = "Book",
    y = "Percentage of Total Words",
    fill = "Point of View"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(pov_plot)
```

Perspective stable index

```{r}
 # POV Stability Analysis (Window-based approach)
# ---------------------------------------

# First, check what columns are actually available in your token data
# str(gaiman_corpus$token_level)

# Adjust the analysis to work with your corpus structure
pov_stability <- gaiman_corpus$token_level %>%
  # Filter for pronoun tokens using token_lower instead of upos
  filter(token_lower %in% c(
    # First person
    "i", "me", "my", "mine", "myself", "we", "us", "our", "ours", "ourselves",
    # Second person
    "you", "your", "yours", "yourself", "yourselves",
    # Third person
    "he", "him", "his", "himself", "she", "her", "hers", "herself",
    "it", "its", "itself", "they", "them", "their", "theirs", "themselves"
  )) %>%
  # Add perspective classification
  mutate(
    perspective = case_when(
      token_lower %in% c("i", "me", "my", "mine", "myself", "we", "us", "our", "ours", "ourselves") ~ "first",
      token_lower %in% c("you", "your", "yours", "yourself", "yourselves") ~ "second",
      TRUE ~ "third"
    )
  ) %>%
  # Group by book and sentence
  group_by(book_id, sentence_id) %>%
  # Determine predominant perspective for each sentence
  summarize(
    perspective = case_when(
      sum(perspective == "first") > 0 ~ "first",
      sum(perspective == "second") > 0 ~ "second",
      sum(perspective == "third") > 0 ~ "third",
      TRUE ~ "none"  # Fallback (shouldn't happen given our filter)
    ),
    .groups = "drop"
  ) %>%
  # Group sentences into windows to avoid name-pronoun alternation issues
  group_by(book_id) %>%
  mutate(
    # Create perspective windows (groups of consecutive sentences)
    window_id = ceiling(row_number() / 5)
  ) %>%
  # Get dominant perspective for each window
  group_by(book_id, window_id) %>%
  summarize(
    dominant_perspective = names(which.max(table(perspective))),
    .groups = "drop"
  ) %>%
  # Track shifts in dominant perspective
  group_by(book_id) %>%
  mutate(
    prev_perspective = lag(dominant_perspective),
    is_shift = dominant_perspective != prev_perspective & !is.na(prev_perspective)
  ) %>%
  # Calculate stability index and perspective composition
  summarize(
    windows = n(),
    shifts = sum(is_shift, na.rm = TRUE),
    stability_index = 1 - (shifts / (windows - 1)),  # Higher = more stable
    pct_first = mean(dominant_perspective == "first", na.rm = TRUE) * 100,
    pct_second = mean(dominant_perspective == "second", na.rm = TRUE) * 100,
    pct_third = mean(dominant_perspective == "third", na.rm = TRUE) * 100,
    .groups = "drop"
  )

# Get book titles for better visualization
pov_stability <- pov_stability %>%
  left_join(
    gaiman_corpus$document_level %>% 
      select(book_id, title) %>% 
      distinct(),
    by = "book_id"
  )

# Create perspective stability visualization
stability_plot <- ggplot(pov_stability, 
                        aes(x = reorder(book_id, stability_index),
                            y = stability_index)) +
  geom_col(fill = "#6D9EC1") +
  geom_text(aes(label = sprintf("%.2f", stability_index)), 
            vjust = -0.5, size = 3) +
  labs(
    title = "Narrative Perspective Stability in Gaiman's Works",
    subtitle = "Higher values indicate more consistent perspective throughout the work",
    x = "Book",
    y = "Perspective Stability Index"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Create perspective composition visualization
composition_data <- pov_stability %>%
  select(book_id, pct_first, pct_second, pct_third) %>%
  pivot_longer(
    cols = c(pct_first, pct_second, pct_third),
    names_to = "perspective",
    values_to = "percentage"
  ) %>%
  mutate(
    perspective = case_when(
      perspective == "pct_first" ~ "First Person",
      perspective == "pct_second" ~ "Second Person",
      perspective == "pct_third" ~ "Third Person"
    )
  )

composition_plot <- ggplot(composition_data,
                          aes(x = reorder(book_id, percentage, sum),
                              y = percentage,
                              fill = perspective)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("First Person" = "#E57373",
                               "Second Person" = "#FFB74D",
                               "Third Person" = "#81C784")) +
  labs(
    title = "Narrative Perspective Composition in Gaiman's Works",
    subtitle = "Percentage of narrative told in each perspective",
    x = "Book",
    y = "Percentage",
    fill = "Perspective"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display both visualizations
print(stability_plot)
print(composition_plot)
```

#### 2. Modality

-   use narrative function categorisation

```{r}
# Modal Verb Analysis by Narrative Function
# ---------------------------------------

# Prepare book texts - create if not already available
book_texts <- gaiman_corpus$document_level %>%
  group_by(book_id) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup()

# Corrected function to categorize modal verbs by narrative function
analyze_modal_functions <- function(text) {
  # Define patterns for modal categories
  world_building_pattern <- "\\b(must|should|have to|has to|had to|always|never)\\b"
  possibility_pattern <- "\\b(may|might|could|can|maybe|perhaps)\\b"
  hypothetical_pattern <- "\\b(would|if|whether)\\b"
  intent_pattern <- "\\b(will|shall|going to|want to)\\b"
  
  # Count matches for each category
  world_count <- str_count(tolower(text), world_building_pattern)
  possibility_count <- str_count(tolower(text), possibility_pattern)
  hypothetical_count <- str_count(tolower(text), hypothetical_pattern)
  intent_count <- str_count(tolower(text), intent_pattern)
  
  # Return named vector of counts
  return(c(
    world_building = world_count,
    possibility = possibility_count,
    hypothetical = hypothetical_count,
    intent = intent_count
  ))
}

# Apply function to each book
modal_results <- book_texts %>%
  rowwise() %>%
  mutate(
    # Apply the function to get modal counts
    modal_counts = list(analyze_modal_functions(text)),
    # Extract individual counts
    world_building = modal_counts["world_building"],
    possibility = modal_counts["possibility"],
    hypothetical = modal_counts["hypothetical"],
    intent = modal_counts["intent"],
    # Get total word count for normalization
    total_words = str_count(text, "\\S+"),
    # Normalize per 1000 words
    world_building_per_k = (world_building / total_words) * 1000,
    possibility_per_k = (possibility / total_words) * 1000,
    hypothetical_per_k = (hypothetical / total_words) * 1000,
    intent_per_k = (intent / total_words) * 1000,
    # Calculate authority ratio (world_building + intent vs. possibility + hypothetical)
    authority_ratio = (world_building + intent) / (possibility + hypothetical)
  ) %>%
  # Select only needed columns
  select(book_id, 
         world_building, possibility, hypothetical, intent, 
         world_building_per_k, possibility_per_k, hypothetical_per_k, intent_per_k,
         authority_ratio)

```

```{r}
# Get book titles for visualization
modal_results <- modal_results %>%
  left_join(
    gaiman_corpus$document_level %>% 
      select(book_id, title) %>% 
      distinct(),
    by = "book_id"
  )

# Prepare data for visualization
modal_long <- modal_results %>%
  select(book_id, title, ends_with("_per_k")) %>%
  pivot_longer(
    cols = ends_with("_per_k"),
    names_to = "modal_type",
    values_to = "frequency_per_k"
  ) %>%
  # Clean up category names
  mutate(
    modal_type = case_when(
      modal_type == "world_building_per_k" ~ "World Building",
      modal_type == "possibility_per_k" ~ "Possibility",
      modal_type == "hypothetical_per_k" ~ "Hypothetical",
      modal_type == "intent_per_k" ~ "Intent"
    )
  )

# Create improved modal verb usage visualization
modal_plot <- ggplot(modal_long,
                    aes(x = reorder(book_id, frequency_per_k, sum),
                        y = frequency_per_k,
                        fill = modal_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("World Building" = "#5C6BC0",
                               "Possibility" = "#26A69A",
                               "Hypothetical" = "#AB47BC",
                               "Intent" = "#EF5350")) +
  labs(
    title = "Modal Verb Usage by Narrative Function",
    subtitle = "Frequency per 1,000 words across Gaiman's works",
    x = "Book",
    y = "Frequency (per 1,000 words)",
    fill = "Modal Function"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(modal_plot)


```

```{r}
# Authority ratio visualization
authority_plot <- ggplot(modal_results,
                        aes(x = reorder(book_id, authority_ratio),
                            y = authority_ratio)) +
  geom_col(fill = "#FF7043") +
  geom_text(aes(label = sprintf("%.2f", authority_ratio)), 
            vjust = -0.5, size = 3) +
  labs(
    title = "Narrative Voice Authority in Gaiman's Works",
    subtitle = "Ratio of authoritative modals (world-building + intent) to uncertain modals (possibility + hypothetical)",
    x = "Book",
    y = "Authority Ratio"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display visualizations

print(authority_plot)
```

#### 4. Combined Analysis

-   consider a way to combine POV and modal verbs

#### Integrated Analysis 

Consider use PCA to visualise the distribution of the ten books on the spectrum of children-adults.\
Or if you have some better idea.

```{r}
# 1. Gather all relevant features into one dataset
# ------------------------------------------------

# Start with book-level metadata as base
book_features <- gaiman_corpus$metadata$book_level %>%
  select(book_id, title)

# Add lexical features
book_features <- book_features %>%
  left_join(
    book_metadata %>% 
      select(book_id, lexical_diversity),
    by = "book_id"
  ) %>%
  # Add word length features
  left_join(
    word_lengh_stats %>% 
      select(book_id, avg_length, short_word_pct, medium_word_pct, long_word_pct),
    by = "book_id"
  ) %>%
  # Add sentence length features
  left_join(
    sentence_length_stats %>% 
      select(book_id, avg_length, short_sent_pct, long_sent_pct),
    by = "book_id",
    suffix = c("_word", "_sentence")
  ) %>%
  # Add syntactic complexity
  left_join(
    book_dependency_stats %>% 
      select(book_id, mean_dep_length),
    by = "book_id"
  ) %>%
  # Add readability scores
  left_join(
    readability_data %>% 
      select(book_id, Flesch.Kincaid),
    by = "book_id"
  ) %>%
  # Add sentiment metrics
  left_join(
    book_sentiment %>% 
      select(book_id, ave_sentiment, positive_pct, negative_pct),
    by = "book_id"
  ) %>%
  # Add profanity metrics (normalized per 1000 words)
  left_join(
    profanity_enhanced %>% 
      select(book_id, mild_per_k, moderate_per_k, strong_per_k),
    by = "book_id"
  ) %>%
  # Add POV metrics
  left_join(
    pov_results %>% 
      select(book_id, first_pct, second_pct, third_pct),
    by = "book_id"
  ) %>%
  # Add modal verb metrics
  left_join(
    modal_results %>% 
      select(book_id, world_building_per_k, possibility_per_k, hypothetical_per_k, intent_per_k, authority_ratio),
    by = "book_id"
  )
```

```{r}
# 2. Clean and prepare features for PCA
# -------------------------------------

# Handle any missing values 
book_features <- book_features %>%
  # Replace NAs with mean values for each feature
  mutate(across(where(is.numeric), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Scale features (important for PCA)
pca_data <- book_features %>%
  select(-book_id, -title) %>%  # Remove non-numeric columns
  scale()

# Add row names for the PCA plot
rownames(pca_data) <- book_features$book_id

# 3. Perform PCA
# --------------

# Conduct PCA
pca_result <- prcomp(pca_data, center = TRUE, scale. = TRUE)

# 4. Create enhanced visualization
# --------------------------------

# Add PCA coordinates to original book data
pca_data_viz <- as_tibble(pca_result$x) %>%
  bind_cols(book_features %>% select(book_id, title))

# Calculate variance explained by each PC
variance_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2) * 100

# Create PCA plot with ggplot2
pca_plot <- ggplot(pca_data_viz, aes(x = PC1, y = PC2, label = book_id)) +
  # Use a single color for points to avoid bias
  geom_point(size = 4, color = "#5B9BD5", alpha = 0.7) +
  geom_text_repel(
    box.padding = 0.5,
    point.padding = 0.3,
    segment.color = "grey50",
    force = 3
  ) +
  labs(
    title = "Stylistic Features of Gaiman's Works",
    subtitle = paste0("PC1 explains ", round(variance_explained[1], 1), 
                      "% and PC2 explains ", round(variance_explained[2], 1), "% of variance"),
    x = paste0("Principal Component 1 (", round(variance_explained[1], 1), "%)"),
    y = paste0("Principal Component 2 (", round(variance_explained[2], 1), "%)")
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# 5. Add feature contribution arrows
# ----------------------------------

# Extract loadings
loadings <- pca_result$rotation

# Calculate feature contributions to each PC
feature_contrib <- data.frame(
  feature = rownames(loadings),
  PC1_loading = loadings[,1],
  PC2_loading = loadings[,2],
  PC1_contrib = loadings[,1]^2 / sum(loadings[,1]^2) * 100,
  PC2_contrib = loadings[,2]^2 / sum(loadings[,2]^2) * 100,
  total_contrib = (loadings[,1]^2 / sum(loadings[,1]^2) + 
                    loadings[,2]^2 / sum(loadings[,2]^2)) * 50  # Average contribution
)

# Select top contributing features for clearer visualization
top_features <- feature_contrib %>%
  arrange(desc(total_contrib)) %>%
  head(12) %>%
  pull(feature)

# Filter to top features
top_loadings <- feature_contrib %>%
  filter(feature %in% top_features)

# Scale factor for arrows
arrow_scale <- 5

# Add arrows to the plot for top contributing features
pca_plot_with_arrows <- pca_plot +
  geom_segment(
    data = top_loadings,
    aes(x = 0, y = 0, 
        xend = PC1_loading * arrow_scale, 
        yend = PC2_loading * arrow_scale),
    arrow = arrow(length = unit(0.3, "cm")),
    color = "#E76F51",
    linewidth = 0.7
  ) +
  geom_text_repel(
    data = top_loadings,
    aes(x = PC1_loading * arrow_scale * 1.1, 
        y = PC2_loading * arrow_scale * 1.1, 
        label = feature),
    color = "#2A4D69",
    size = 3.5,
    fontface = "bold",
    inherit.aes = FALSE
  )

# 6. Add interpretative annotations based on factor loadings
# ---------------------------------------------------------

# Identify primary directions of key features
complexity_features <- c("Flesch.Kincaid", "mean_dep_length", "long_sent_pct", "long_word_pct")
simplicity_features <- c("short_sent_pct", "short_word_pct")

# Calculate average loading direction for complexity and simplicity
complexity_direction <- feature_contrib %>%
  filter(feature %in% complexity_features) %>%
  summarise(
    PC1_loading_avg = mean(PC1_loading),
    PC2_loading_avg = mean(PC2_loading)
  )

simplicity_direction <- feature_contrib %>%
  filter(feature %in% simplicity_features) %>%
  summarise(
    PC1_loading_avg = mean(PC1_loading),
    PC2_loading_avg = mean(PC2_loading)
  )

# Add annotations based on these calculated directions
pca_plot_final <- pca_plot_with_arrows +
  # Only add directional annotation if there's a clear pattern
  {
    # Check if complexity is primarily associated with PC1
    if(abs(complexity_direction$PC1_loading_avg) > abs(complexity_direction$PC2_loading_avg) * 1.5) {
      if(complexity_direction$PC1_loading_avg > 0) {
        annotate(
          "text", 
          x = max(pca_data_viz$PC1) * 0.85, 
          y = 0, 
          label = "→ Higher linguistic complexity", 
          hjust = 1,
          size = 3.5,
          fontface = "italic",
          color = "#2A4D69"
        )
      } else {
        annotate(
          "text", 
          x = min(pca_data_viz$PC1) * 0.85, 
          y = 0, 
          label = "← Higher linguistic complexity", 
          hjust = 0,
          size = 3.5,
          fontface = "italic",
          color = "#2A4D69"
        )
      }
    }
  }

# 7. Calculate potential clusters
# ------------------------------

# Perform k-means clustering with k=3 (but not used for coloring until interpretation)
set.seed(123) # For reproducibility
kmeans_result <- kmeans(pca_data_viz[, c("PC1", "PC2")], centers = 3, nstart = 25)

# Add cluster information to the data (for reference only)
pca_data_viz$cluster <- as.factor(kmeans_result$cluster)

# Create a data frame mapping book_id to cluster
book_clusters <- data.frame(
  book_id = pca_data_viz$book_id,
  cluster = pca_data_viz$cluster
)

# 8. Print key insights
# --------------------

# Top contributors to PC1
cat("Top contributors to PC1 (potential complexity dimension):\n")
print(feature_contrib %>% 
        arrange(desc(abs(PC1_loading))) %>% 
        head(8) %>% 
        select(feature, PC1_loading, PC1_contrib))

# Top contributors to PC2
cat("\nTop contributors to PC2 (second dimension):\n")
print(feature_contrib %>% 
        arrange(desc(abs(PC2_loading))) %>% 
        head(8) %>% 
        select(feature, PC2_loading, PC2_contrib))

# Show the cluster memberships (without labeling the clusters)
cat("\nBooks grouped by similar linguistic features:\n")
for(i in 1:length(unique(book_clusters$cluster))) {
  cat(paste0("Group ", i, ": ", 
            paste(book_clusters$book_id[book_clusters$cluster == i], collapse=", "), 
            "\n"))
}

# Display the final PCA plot
print(pca_plot_final)


```

```{r}
# 9. Create a heatmap visualization of feature correlations with PCs
# -----------------------------------------------------------------

# Get correlation of features with PCs
feature_pc_correlation <- data.frame(
  feature = rownames(loadings),
  PC1 = loadings[,1],
  PC2 = loadings[,2]
) %>%
  # Focus on top contributors
  filter(feature %in% top_features) %>%
  # Prepare for ggplot
  pivot_longer(cols = c(PC1, PC2), 
               names_to = "component", 
               values_to = "correlation")

# Create heatmap
feature_heatmap <- ggplot(feature_pc_correlation, 
                         aes(x = component, 
                             y = reorder(feature, abs(correlation)), 
                             fill = correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "#4575B4", mid = "white", high = "#D73027", 
                      midpoint = 0, limits = c(-1, 1)) +
  labs(
    title = "Feature Loadings on Principal Components",
    x = "Principal Component", 
    y = "Feature",
    fill = "Correlation"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(hjust = 1),
    panel.grid = element_blank()
  )

print(feature_heatmap)
```

### Discussion 

### Conclusion 

### Reference
